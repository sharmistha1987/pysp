These can be set as per requirement in spark-submit of spark session config:

spark.sql.shuffle.partitions=50/300/any number --specifying number of partitons when shuffling data for joins or aggregations.
spark.default.parallelism=300/any number --specify default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set explicitly by the user. 
                                         --Note that spark.default.parallelism seems to only be working for raw RDD and is ignored when working with dataframes.
                                         
                                         
driver-memory 12g 
executor-memory 20g 
num-executors 29 
executor-cores 4 
driver-cores 4
