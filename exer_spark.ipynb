{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark=SparkSession.builder.appName('exer_spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Average Friends by Age ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,Will,33,385',\n",
       " '1,Jean-Luc,26,2',\n",
       " '2,Hugh,55,221',\n",
       " '3,Deanna,40,465',\n",
       " '4,Quark,68,21']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_data=sc.textFile('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\fakefriends.csv')\n",
    "f_data.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_numf=f_data.map(lambda x:(x.split(',')[2],x.split(',')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('33', ('385', 1)),\n",
       " ('26', ('2', 1)),\n",
       " ('55', ('221', 1)),\n",
       " ('40', ('465', 1)),\n",
       " ('68', ('21', 1))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_num_map=age_numf.mapValues(lambda x: (x,1))\n",
    "age_num_map.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('33', ('38574471275245356460294243463228410', 12)),\n",
       " ('26', ('228184282381145345293298492269254738312439184', 17)),\n",
       " ('40', ('4652544594071828438934940619817233567261286220', 17)),\n",
       " ('68', ('21264112490481217189206293423', 10)),\n",
       " ('54', ('30725375440744123536939746272442115', 13))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_totalnum=age_num_map.reduceByKey(lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "age_totalnum.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('33', 3.214539272937113e+33),\n",
       " ('26', 1.3422604845949725e+43),\n",
       " ('40', 2.736790937689311e+44),\n",
       " ('68', 2.1264112490481217e+27),\n",
       " ('54', 2.3634904185187785e+33)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_age=age_totalnum.mapValues(lambda x:float(x[0])/x[1])\n",
    "\n",
    "average_age.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordcount of a text file with regex ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-Employment:',\n",
       " 'Building',\n",
       " 'an',\n",
       " 'Internet',\n",
       " 'Business',\n",
       " 'of',\n",
       " 'One',\n",
       " 'Achieving',\n",
       " 'Financial',\n",
       " 'and']"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines=sc.textFile('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\Book.txt')\n",
    "#words=lines.flatMap(lambda x:x.split())\n",
    "#words.take(10) # there are many special characters and blanks (seen thru collect()),so we need to use regex to ensure only valid words are selected\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def regex_pr(line): # to filter special characters\n",
    "    c=re.compile(r'\\W+',re.UNICODE)\n",
    "    return c.split(line.lower())  # after compiling regexp for detecting valid chars apply that to \n",
    "                                  # split line where capitals and smalls are treated equal. \n",
    "    #return re.split(c,line.lower())\n",
    "    #re.split(<pattern,<text>) expects a text s argument.\n",
    "    #re.compile(<pattern>,<flag>) and saving the resulting regular expression object for reuse is more efficient\n",
    "    #when the expression will be used several times in a single program\n",
    "    # re.UNICODE to specify the text has some unicode info . you can also write it as re.U.\n",
    " \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['self',\n",
       " 'employment',\n",
       " 'building',\n",
       " 'an',\n",
       " 'internet',\n",
       " 'business',\n",
       " 'of',\n",
       " 'one',\n",
       " 'achieving',\n",
       " 'financial']"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=lines.flatMap(regex_pr)  \n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('self', 111), ('an', 178), ('internet', 26), ('business', 383), ('of', 970)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_of_words=words.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)\n",
    "count_of_words.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('self', 111)\n",
      "('an', 178)\n",
      "('internet', 26)\n",
      "('business', 383)\n",
      "('of', 970)\n"
     ]
    }
   ],
   "source": [
    "for c in count_of_words.take(5): # printing count of words\n",
    "    print(c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1878, 'you')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_by_count=count_of_words.map(lambda x :(x[1],x[0])).sortByKey(False)\n",
    "sort_by_count.first() # max count and related word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self:111\n",
      "an:178\n",
      "internet:26\n",
      "business:383\n",
      "of:970\n"
     ]
    }
   ],
   "source": [
    "for c in count_of_words.take(5): # formating result as word:count\n",
    "    count = str(c[1])\n",
    "    word = str(c[0])\n",
    "    print(word+':'+count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Most Popular Superhero and coappearances in a Social Graph ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseNames(line):\n",
    "    fields = line.split('\\\"') #split on quotes to get key and name\n",
    "    return (int(fields[0]), fields[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countCoOccurences(line):\n",
    "    #elements = line.strip().split()\n",
    "    elements = line.split()\n",
    "    return (int(elements[0]), len(elements) -1) # to subtract id (key) from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 \"24-HOUR MAN/EMMANUEL\"',\n",
       " '2 \"3-D MAN/CHARLES CHAN\"',\n",
       " '3 \"4-D MAN/MERCURIO\"',\n",
       " '4 \"8-BALL/\"',\n",
       " '5 \"A\"']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\marvel-names.txt\")\n",
    "names.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '24-HOUR MAN/EMMANUEL'),\n",
       " (2, '3-D MAN/CHARLES CHAN'),\n",
       " (3, '4-D MAN/MERCURIO'),\n",
       " (4, '8-BALL/'),\n",
       " (5, 'A'),\n",
       " (6, \"A'YIN\"),\n",
       " (7, 'ABBOTT, JACK'),\n",
       " (8, 'ABCISSA'),\n",
       " (9, 'ABEL'),\n",
       " (10, 'ABOMINATION/EMIL BLO')]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namesRdd = names.map(parseNames)\n",
    "namesRdd.take(10) # contains key of superhero,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5988 748 1722 3752 4655 5743 1872 3413 5527 6368 6085 4319 4728 1636 2397 3364 4001 1614 1819 1585 732 2660 3952 2507 3891 2070 2239 2602 612 1352 5447 4548 1596 5488 1605 5517 11 479 2554 2043 17 865 4292 6312 473 534 1479 6375 4456 ',\n",
       " '5989 4080 4264 4446 3779 2430 2297 6169 3530 3272 4282 6432 2548 4140 185 105 3878 2429 1334 4595 2767 3956 3877 4776 4946 3407 128 269 5775 5121 481 5516 4758 4053 1044 1602 3889 1535 6038 533 3986 ',\n",
       " '5982 217 595 1194 3308 2940 1815 794 1503 5197 859 5096 6039 2664 651 2244 528 284 1449 1097 1172 1092 108 3405 5204 387 4607 4545 3705 4930 1805 4712 4404 247 4754 4427 1845 536 5795 5978 533 3984 6056 ',\n",
       " '5983 1165 3836 4361 1282 716 4289 4646 6300 5084 2397 4454 1913 5861 5485 ',\n",
       " '5980 2731 3712 1587 6084 2472 2546 6313 875 859 323 2664 1469 522 2506 2919 2423 3624 5736 5046 1787 5776 3245 3840 2399 ']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\marvel-graph.txt\")\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5988, 48),\n",
       " (5989, 40),\n",
       " (5982, 42),\n",
       " (5983, 14),\n",
       " (5980, 24),\n",
       " (5981, 17),\n",
       " (5986, 142),\n",
       " (5987, 81),\n",
       " (5984, 41),\n",
       " (5985, 19)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairings = lines.map(countCoOccurences)\n",
    "pairings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(48, 5988),\n",
       " (42, 5982),\n",
       " (24, 5980),\n",
       " (142, 5986),\n",
       " (41, 5984),\n",
       " (13, 6294),\n",
       " (42, 270),\n",
       " (45, 272),\n",
       " (410, 274),\n",
       " (15, 276)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalFriendsByCharacter = pairings.reduceByKey(lambda x, y : x + y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(859, 1933)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostpopular=totalFriendsByCharacter.sortBy(lambda x:x[1],False).first() #sort value for each key in descending order\n",
    "mostpopular #key of superhero,occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1933, 859)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternate way of finding max value of key is using max()\n",
    "\n",
    "flipped = totalFriendsByCharacter.map(lambda x : (x[1],x[0]))\n",
    "flipped.take(10)\n",
    "mostpopular1=flipped.max() # max number of coappearances ,max() operate on key of k,v pair or max(<key function>) where key can be changed\n",
    "mostpopular1 #contains count of friends,key of superhero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular name is CAPTAIN AMERICA with coappearances=1933\n"
     ]
    }
   ],
   "source": [
    "# to get matching name from id (key\n",
    "\n",
    "#for name in namesRdd.collect():\n",
    "    #if mostpopular1[1]==name[0]: # mostpopular[0]==name[0]:\n",
    "       #print(name[1])\n",
    "        \n",
    "#alternate way \n",
    "\n",
    "print('popular name is '+namesRdd.lookup(mostpopular[0])[0]+' with coappearances='+ str(mostpopular[1])) # lookup(<value>),to lookup for a value in rdd. Return list of matches if value found\n",
    "                                                                                                         #[0] after lookup() - to display value of list\n",
    "\n",
    "#print('popular name is '+namesRdd.lookup(mostpopular1[1])[0]+' with coappearances='+ str(mostpopular1[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total-Amount-By-Customer##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\customer-orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['44,8602,37.19',\n",
       " '35,5368,65.89',\n",
       " '2,3391,40.64',\n",
       " '47,6694,14.98',\n",
       " '29,680,13.08']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    customerId = int(fields[0])\n",
    "    itemId = fields[1]\n",
    "    itemAmt = float(fields[2])\n",
    "    return (customerId, itemAmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 4756.890000000001),\n",
       " (2, 5994.59),\n",
       " (70, 5368.249999999999),\n",
       " (14, 4735.030000000001),\n",
       " (42, 5696.840000000002)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#orders = cust.map(parseLine)\n",
    "id_amt=cust.map(lambda line: (int(line.split(',')[0]),float(line.split(',')[2])))\n",
    "totals = id_amt.reduceByKey(lambda x, y: x + y)\n",
    "totals.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4756.89 \t44.00\n",
      "5994.59 \t2.00\n",
      "5368.25 \t70.00\n",
      "4735.03 \t14.00\n",
      "5696.84 \t42.00\n"
     ]
    }
   ],
   "source": [
    "for result in totals.take(5):\n",
    "    print(\"{:.2f}\".format(result[1]),\"\\t{:.2f}\".format(result[0])) #:.2f for formatting in placeholder and then using format() to display result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min/Max -Temperatures per station ID##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ITE00100554,18000101,TMAX,-75,,,E,',\n",
       " 'ITE00100554,18000101,TMIN,-148,,,E,',\n",
       " 'GM000010962,18000101,PRCP,0,,,E,',\n",
       " 'EZE00100082,18000101,TMAX,-86,,,E,',\n",
       " 'EZE00100082,18000101,TMIN,-135,,,E,']"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\1800.csv\")\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To take off required fields and assigning them to variable,create function where split line on a pattern and assign variables to reqd fields.\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 'TMAX', 18.5),\n",
       " ('ITE00100554', 'TMIN', 5.359999999999999),\n",
       " ('GM000010962', 'PRCP', 32.0),\n",
       " ('EZE00100082', 'TMAX', 16.52),\n",
       " ('EZE00100082', 'TMIN', 7.699999999999999)]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedLines=lines.map(parseLine) # use the function to actual line element of rdd\n",
    "parsedLines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 'TMIN', 5.359999999999999),\n",
       " ('EZE00100082', 'TMIN', 7.699999999999999),\n",
       " ('ITE00100554', 'TMIN', 9.5),\n",
       " ('EZE00100082', 'TMIN', 8.599999999999998),\n",
       " ('ITE00100554', 'TMIN', 23.72)]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1]) # check if TMIN as a 1st pos value in each element of rdd,hence use filter() to filter out such elements of rdd\n",
    "minTemps.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 5.359999999999999),\n",
       " ('EZE00100082', 7.699999999999999),\n",
       " ('ITE00100554', 9.5),\n",
       " ('EZE00100082', 8.599999999999998),\n",
       " ('ITE00100554', 23.72)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_temp=minTemps.map(lambda x:(x[0],x[2]))\n",
    "id_temp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 5.359999999999999), ('EZE00100082', 7.699999999999999)]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTemps = id_temp.reduceByKey(lambda x,y: x if x<y else y) # to calculate min of values (temps) per key\n",
    "minTemps.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station Id ITE00100554 has min temp of 5.36F\n",
      "station Id EZE00100082 has min temp of 7.70F\n",
      "\n",
      "station Id ITE00100554 has lowest temp of 5.36F of all stations\n"
     ]
    }
   ],
   "source": [
    "for m in minTemps.collect(): #display min temperature per station\n",
    "    print('station Id '+m[0]+' has min temp of {:.2f}F'.format(m[1]))\n",
    "    \n",
    "#to get lowest temperature and related station ID and of all stations.\n",
    "\n",
    "m=minTemps.min(lambda x:x[1]) # to get min temperature pair\n",
    "\n",
    "print('\\nstation Id '+m[0]+' has lowest temp of {:.2f}F '.format(m[1])+'of all stations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
