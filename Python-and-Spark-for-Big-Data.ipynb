{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# May take a while locally\n",
    "spark = SparkSession.builder.appName(\"'PySparkShell\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.SparkConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\appl_stock.csv',inferSchema=True,header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              Open|\n",
      "+------------------+\n",
      "|        213.429998|\n",
      "|        214.599998|\n",
      "|        214.379993|\n",
      "|            211.75|\n",
      "|        210.299994|\n",
      "|212.79999700000002|\n",
      "|209.18999499999998|\n",
      "|        207.870005|\n",
      "|210.11000299999998|\n",
      "|210.92999500000002|\n",
      "|        208.330002|\n",
      "|        214.910006|\n",
      "|        212.079994|\n",
      "|206.78000600000001|\n",
      "|202.51000200000001|\n",
      "|205.95000100000001|\n",
      "|        206.849995|\n",
      "|        204.930004|\n",
      "|        201.079996|\n",
      "|192.36999699999998|\n",
      "|        195.909998|\n",
      "|        195.169994|\n",
      "|        196.730003|\n",
      "|192.63000300000002|\n",
      "|        195.690006|\n",
      "|        196.419996|\n",
      "|        195.889997|\n",
      "|        194.880001|\n",
      "|        198.109995|\n",
      "|        201.940002|\n",
      "+------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "#df.select(col('Open')).show()\n",
    "#df.select('Open').show() # by default shows 20 rows\n",
    "df.select('Open').show(30,True) # if True, then truncate decimal places up to 20 digit max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|format_number(Open, 2)|\n",
      "+----------------------+\n",
      "|                213.43|\n",
      "|                214.60|\n",
      "|                214.38|\n",
      "|                211.75|\n",
      "|                210.30|\n",
      "|                212.80|\n",
      "|                209.19|\n",
      "|                207.87|\n",
      "|                210.11|\n",
      "|                210.93|\n",
      "|                208.33|\n",
      "|                214.91|\n",
      "|                212.08|\n",
      "|                206.78|\n",
      "|                202.51|\n",
      "|                205.95|\n",
      "|                206.85|\n",
      "|                204.93|\n",
      "|                201.08|\n",
      "|                192.37|\n",
      "+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(format_number('Open',2)).show() # format to two decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 4, 0, 0), Open=213.429998, High=214.499996, Low=212.38000099999996, Close=214.009998, Volume=123432400, Adj Close=27.727039),\n",
       " Row(Date=datetime.datetime(2010, 1, 5, 0, 0), Open=214.599998, High=215.589994, Low=213.249994, Close=214.379993, Volume=150476200, Adj Close=27.774976000000002),\n",
       " Row(Date=datetime.datetime(2010, 1, 6, 0, 0), Open=214.379993, High=215.23, Low=210.750004, Close=210.969995, Volume=138040000, Adj Close=27.333178000000004),\n",
       " Row(Date=datetime.datetime(2010, 1, 7, 0, 0), Open=211.75, High=212.000006, Low=209.050005, Close=210.58, Volume=119282800, Adj Close=27.28265),\n",
       " Row(Date=datetime.datetime(2010, 1, 8, 0, 0), Open=210.299994, High=212.000006, Low=209.06000500000002, Close=211.98000499999998, Volume=111902700, Adj Close=27.464034)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|summary|              Open|              High|               Low|            Close|             Volume|         Adj Close|\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|  count|              1762|              1762|              1762|             1762|               1762|              1762|\n",
      "|   mean| 313.0763111589103| 315.9112880164581| 309.8282405079457|312.9270656379113|9.422577587968218E7| 75.00174115607275|\n",
      "| stddev|185.29946803981522|186.89817686485767|183.38391664371008|185.1471036170943|6.020518776592709E7| 28.57492972179906|\n",
      "|    min|              90.0|         90.699997|         89.470001|        90.279999|           11475900|         24.881912|\n",
      "|    max|        702.409988|        705.070023|        699.569977|       702.100021|          470249500|127.96609099999999|\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11 00:00:00|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12 00:00:00|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13 00:00:00|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14 00:00:00|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15 00:00:00|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19 00:00:00|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20 00:00:00|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21 00:00:00|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22 00:00:00|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25 00:00:00|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26 00:00:00|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27 00:00:00|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28 00:00:00|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01 00:00:00|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Close<500).show()\n",
    "#df.filter(df['close']<500).show()\n",
    "#df.filter(\"close<500\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+\n",
      "|               date|              open|             close|\n",
      "+-------------------+------------------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|        214.009998|\n",
      "|2010-01-05 00:00:00|        214.599998|        214.379993|\n",
      "|2010-01-06 00:00:00|        214.379993|        210.969995|\n",
      "|2010-01-07 00:00:00|            211.75|            210.58|\n",
      "|2010-01-08 00:00:00|        210.299994|211.98000499999998|\n",
      "|2010-01-11 00:00:00|212.79999700000002|210.11000299999998|\n",
      "|2010-01-12 00:00:00|209.18999499999998|        207.720001|\n",
      "|2010-01-13 00:00:00|        207.870005|        210.650002|\n",
      "|2010-01-14 00:00:00|210.11000299999998|            209.43|\n",
      "|2010-01-15 00:00:00|210.92999500000002|            205.93|\n",
      "|2010-01-19 00:00:00|        208.330002|        215.039995|\n",
      "|2010-01-20 00:00:00|        214.910006|            211.73|\n",
      "|2010-01-21 00:00:00|        212.079994|        208.069996|\n",
      "|2010-01-22 00:00:00|206.78000600000001|            197.75|\n",
      "|2010-01-25 00:00:00|202.51000200000001|        203.070002|\n",
      "|2010-01-26 00:00:00|205.95000100000001|        205.940001|\n",
      "|2010-01-27 00:00:00|        206.849995|        207.880005|\n",
      "|2010-01-28 00:00:00|        204.930004|        199.289995|\n",
      "|2010-01-29 00:00:00|        201.079996|        192.060003|\n",
      "|2010-02-01 00:00:00|192.36999699999998|        194.729998|\n",
      "+-------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('date','open','close').filter('close<500').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|               Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
      "|2010-01-28 00:00:00|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|               Date|              Open|      High|       Low|     Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|207.499996|    197.16|    197.75|220441900|         25.620401|\n",
      "|2010-01-28 00:00:00|        204.930004|205.500004|198.699995|199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|202.199995|190.250002|192.060003|311488100|         24.883208|\n",
      "+-------------------+------------------+----------+----------+----------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure to add in the parenthesis separating the statements!\n",
    "\n",
    "df.filter((df[\"close\"] < 200) & (df['open'] > 200)).show() # cannot be 'and' or '&&'\n",
    "#df.filter( (df[\"Close\"] < 200) | (df['Open'] > 200) ).show()\n",
    "#df.filter( (df[\"Close\"] < 200) & ~(df['Open'] < 200) ).show()# cannot be '!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "|               Date|              Open|      High|   Low| Close|   Volume|Adj Close|\n",
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|207.499996|197.16|197.75|220441900|25.620401|\n",
      "+-------------------+------------------+----------+------+------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('low==197.16').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.datetime(2010, 1, 22, 0, 0), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter('low==197.16').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df.filter('low==197.16').collect()\n",
    "print(type(x))\n",
    "y=type(x[0])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2010, 1, 22, 0, 0), Open=206.78000600000001, High=207.499996, Low=197.16, Close=197.75, Volume=220441900, Adj Close=25.620401)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r=x[0]\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010-01-22 00:00:00\n",
      "206.78000600000001\n",
      "207.499996\n",
      "197.16\n",
      "197.75\n",
      "220441900\n",
      "25.620401\n"
     ]
    }
   ],
   "source": [
    "for i in x[0]: # loop through row object will result in fetching values in row object\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Date': datetime.datetime(2010, 1, 22, 0, 0),\n",
       " 'Open': 206.78000600000001,\n",
       " 'High': 207.499996,\n",
       " 'Low': 197.16,\n",
       " 'Close': 197.75,\n",
       " 'Volume': 220441900,\n",
       " 'Adj Close': 25.620401}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.asDict() # convert row object of key=value to dictionary of key:value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-QN77D5V.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x282fff829e8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|summary|              Open|              High|               Low|            Close|             Volume|         Adj Close|\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|  count|              1762|              1762|              1762|             1762|               1762|              1762|\n",
      "|   mean| 313.0763111589103| 315.9112880164581| 309.8282405079457|312.9270656379113|9.422577587968218E7| 75.00174115607275|\n",
      "| stddev|185.29946803981522|186.89817686485767|183.38391664371008|185.1471036170943|6.020518776592709E7| 28.57492972179906|\n",
      "|    min|              90.0|         90.699997|         89.470001|        90.279999|           11475900|         24.881912|\n",
      "|    max|        702.409988|        705.070023|        699.569977|       702.100021|          470249500|127.96609099999999|\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['summary', 'open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_sum=df.describe()\n",
    "#df_sum.printSchema()\n",
    "df_sum.withColumnRenamed('Open','open').columns # rename column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|summary|              Open|              High|               Low|            Close|             Volume|         Adj Close|\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|  count|              1762|              1762|              1762|             1762|               1762|              1762|\n",
      "|   mean| 313.0763111589103| 315.9112880164581| 309.8282405079457|312.9270656379113|9.422577587968218E7| 75.00174115607275|\n",
      "| stddev|185.29946803981522|186.89817686485767|183.38391664371008|185.1471036170943|6.020518776592709E7| 28.57492972179906|\n",
      "|    min|              90.0|         90.699997|         89.470001|        90.279999|           11475900|         24.881912|\n",
      "|    max|        702.409988|        705.070023|        699.569977|       702.100021|          470249500|127.96609099999999|\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "\n",
      "+-------+--------+--------+--------+--------+---------+\n",
      "|summary| Open_op|    High|     Low|   Close|   Volume|\n",
      "+-------+--------+--------+--------+--------+---------+\n",
      "|  count|1,762.00|1,762.00|1,762.00|1,762.00|     1762|\n",
      "|   mean|  313.08|  315.91|  309.83|  312.93|     null|\n",
      "| stddev|  185.30|  186.90|  183.38|  185.15|     null|\n",
      "|    min|   90.00|   90.70|   89.47|   90.28| 11475900|\n",
      "|    max|  702.41|  705.07|  699.57|  702.10|470249500|\n",
      "+-------+--------+--------+--------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df.describe()\n",
    "result.show()\n",
    "result.select(result['summary'],\n",
    "              format_number(result['Open'].cast('float'),2).alias('Open_op'), # alias of column after operation on a column\n",
    "              format_number(result['High'].cast('float'),2).alias('High'),\n",
    "              format_number(result['Low'].cast('float'),2).alias('Low'),\n",
    "              format_number(result['Close'].cast('float'),2).alias('Close'),\n",
    "              result['Volume'].cast('int').alias('Volume')\n",
    "             ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(format_number(result['Open'].cast('float'),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV ratio|\n",
      "+--------------------+\n",
      "|1.737793286041590...|\n",
      "|1.432718223878593...|\n",
      "|1.559185743262822...|\n",
      "|1.777288980473295...|\n",
      "|1.894503045949740...|\n",
      "|1.843239827133528...|\n",
      "|1.411500428288146...|\n",
      "|1.392525367557254...|\n",
      "|1.944679270213955...|\n",
      "|1.424753661031169E-6|\n",
      "|1.179111006515548...|\n",
      "|1.408471832522860...|\n",
      "|1.402998948951121...|\n",
      "|9.412910884908904E-7|\n",
      "|7.683215757986584E-7|\n",
      "|4.578412734118504E-7|\n",
      "|4.889907419641508E-7|\n",
      "|7.004672644896166E-7|\n",
      "|6.491419575900331E-7|\n",
      "|1.045505632661596E-6|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\appl_stock.csv',inferSchema=True,header=True)\n",
    "df_new = df.withColumn(\"HV ratio\",df[\"High\"]/df[\"Volume\"])\n",
    "df_new.select(\"HV ratio\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|        213.429998|        214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|        214.599998|        215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|        214.379993|            215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|            211.75|        212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|        210.299994|        212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "|2010-01-11 00:00:00|212.79999700000002|        213.000002|        208.450005|210.11000299999998|115557400|         27.221758|\n",
      "|2010-01-12 00:00:00|209.18999499999998|209.76999500000002|        206.419998|        207.720001|148614900|          26.91211|\n",
      "|2010-01-13 00:00:00|        207.870005|210.92999500000002|        204.099998|        210.650002|151473000|          27.29172|\n",
      "|2010-01-14 00:00:00|210.11000299999998|210.45999700000002|        209.020004|            209.43|108223500|         27.133657|\n",
      "|2010-01-15 00:00:00|210.92999500000002|211.59999700000003|        205.869999|            205.93|148516900|26.680197999999997|\n",
      "|2010-01-19 00:00:00|        208.330002|215.18999900000003|        207.240004|        215.039995|182501900|27.860484999999997|\n",
      "|2010-01-20 00:00:00|        214.910006|        215.549994|        209.500002|            211.73|153038200|         27.431644|\n",
      "|2010-01-21 00:00:00|        212.079994|213.30999599999998|        207.210003|        208.069996|152038600|         26.957455|\n",
      "|2010-01-22 00:00:00|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-25 00:00:00|202.51000200000001|        204.699999|        200.190002|        203.070002|266424900|26.309658000000002|\n",
      "|2010-01-26 00:00:00|205.95000100000001|        213.710005|        202.580004|        205.940001|466777500|         26.681494|\n",
      "|2010-01-27 00:00:00|        206.849995|            210.58|        199.530001|        207.880005|430642100|26.932840000000002|\n",
      "|2010-01-28 00:00:00|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01 00:00:00|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               Date|\n",
      "+-------------------+\n",
      "|2012-09-21 00:00:00|\n",
      "|2012-09-19 00:00:00|\n",
      "|2012-09-18 00:00:00|\n",
      "|2012-09-20 00:00:00|\n",
      "|2012-09-17 00:00:00|\n",
      "|2012-09-14 00:00:00|\n",
      "|2012-09-24 00:00:00|\n",
      "|2012-09-25 00:00:00|\n",
      "|2012-09-13 00:00:00|\n",
      "|2012-09-10 00:00:00|\n",
      "|2012-09-07 00:00:00|\n",
      "|2012-09-27 00:00:00|\n",
      "|2012-09-28 00:00:00|\n",
      "|2012-08-27 00:00:00|\n",
      "|2012-09-06 00:00:00|\n",
      "|2012-08-29 00:00:00|\n",
      "|2012-10-01 00:00:00|\n",
      "|2012-09-05 00:00:00|\n",
      "|2012-08-28 00:00:00|\n",
      "|2012-09-04 00:00:00|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Date').orderBy('High',ascending=False).show() #descending order\n",
    "#df.select('Date').orderBy('High').desc().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date=datetime.datetime(2012, 9, 21, 0, 0))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Date').orderBy('High',ascending=False).head()\n",
    "#df.select('Date').orderBy('High',ascending=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2012, 9, 21, 0, 0)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select('Date').orderBy('High',ascending=False).head()[0]  #Row() is like list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       mean_close|\n",
      "+-----------------+\n",
      "|312.9270656379113|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(mean('Close').alias('mean_close')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-------------------+\n",
      "|        avg(Open)|       avg(Close)|        avg(Volume)|\n",
      "+-----------------+-----------------+-------------------+\n",
      "|313.0763111589103|312.9270656379113|9.422577587968218E7|\n",
      "+-----------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().mean(\"Open\",\"Close\",\"Volume\").show() # mean of columns (separated by comma)- (groupBy() having no column name means consider entor set as a group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+-----------------+\n",
      "|        avg(Volume)|       avg(Close)|        avg(Open)|\n",
      "+-------------------+-----------------+-----------------+\n",
      "|9.422577587968218E7|312.9270656379113|313.0763111589103|\n",
      "+-------------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().agg({\"Open\":'mean',\"Close\":'mean',\"Volume\":'mean'}).show() # mean of list of columns using agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+-----------------+-------------------+-----------------+\n",
      "|        avg(Open)|        avg(High)|         avg(Low)|       avg(Close)|        avg(Volume)|   avg(Adj Close)|\n",
      "+-----------------+-----------------+-----------------+-----------------+-------------------+-----------------+\n",
      "|313.0763111589103|315.9112880164581|309.8282405079457|312.9270656379113|9.422577587968218E7|75.00174115607275|\n",
      "+-----------------+-----------------+-----------------+-----------------+-------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().mean().show() # to do mean of all columns in data frame without specifying all columnn names in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(max(df[\"Volume\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(Volume)|\n",
      "+-----------+\n",
      "|  470249500|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(max('Volume')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "| min_vol|  max_vol|\n",
      "+--------+---------+\n",
      "|11475900|470249500|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(min(\"Volume\").alias('min_vol'),max(\"Volume\").alias('max_vol')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = sc.parallelize([[25,'Prem','1.2034'],[20,'Kate','2.4567']]).toDF([\"age\",\"name\",\"expense\"]) # creating dataframe from rdd using toDF()\n",
    "#df1.show()\n",
    "#df1.registerTempTable(\"Sample\")\n",
    "df1.createOrReplaceTempView('Sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('drop table sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age', 'name', 'expense']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+\n",
      "|age|name|expense|\n",
      "+---+----+-------+\n",
      "| 25|Prem|   1.20|\n",
      "| 20|Kate|   2.46|\n",
      "+---+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select age,name,cast(expense as decimal(5,2)) from Sample').show() #cast decimal value to certain number of places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+\n",
      "|age|name|expense|\n",
      "+---+----+-------+\n",
      "| 26|Prem| 1.2034|\n",
      "| 21|Kate| 2.4567|\n",
      "+---+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn('age',df1.age+1).show() # transformation on a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+\n",
      "|age|name|expense|\n",
      "+---+----+-------+\n",
      "| 25|Prem|   1.20|\n",
      "| 20|Kate|   2.46|\n",
      "+---+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn('expense',format_number(df1.expense.cast('double'),2)).show() # withColumn and cast function to change column type in a dataframe without using spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: bigint, name: string, expense: string]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([[31,'sam','1.2034'],[36,'sumit','2.4567']]) # creating data frame from rdd and schema in createdataframe()\n",
    "df2=spark.createDataFrame(rdd2,[\"age\",\"name\",\"expense\"])\n",
    "df2.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+\n",
      "|age| name|expense|\n",
      "+---+-----+-------+\n",
      "| 31|  sam| 1.2034|\n",
      "| 36|sumit| 2.4567|\n",
      "+---+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|               Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04 00:00:00|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05 00:00:00|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06 00:00:00|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07 00:00:00|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08 00:00:00|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+-------------------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|               Date|              Open|              High|               Low|             Close|   Volume|         Adj Close|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "|2010-01-22 00:00:00|206.78000600000001|        207.499996|            197.16|            197.75|220441900|         25.620401|\n",
      "|2010-01-28 00:00:00|        204.930004|        205.500004|        198.699995|        199.289995|293375600|25.819922000000002|\n",
      "|2010-01-29 00:00:00|        201.079996|        202.199995|        190.250002|        192.060003|311488100|         24.883208|\n",
      "|2010-02-01 00:00:00|192.36999699999998|             196.0|191.29999899999999|        194.729998|187469100|         25.229131|\n",
      "|2010-02-02 00:00:00|        195.909998|        196.319994|193.37999299999998|        195.859997|174585600|25.375532999999997|\n",
      "|2010-02-03 00:00:00|        195.169994|        200.200003|        194.420004|        199.229994|153832000|25.812148999999998|\n",
      "|2010-02-04 00:00:00|        196.730003|        198.370001|        191.570005|        192.050003|189413000|         24.881912|\n",
      "|2010-02-05 00:00:00|192.63000300000002|             196.0|        190.850002|        195.460001|212576700|25.323710000000002|\n",
      "|2010-02-08 00:00:00|        195.690006|197.88000300000002|        193.999994|194.11999699999998|119567700|           25.1501|\n",
      "|2010-02-09 00:00:00|        196.419996|        197.499994|        194.749998|196.19000400000002|158221700|         25.418289|\n",
      "+-------------------+------------------+------------------+------------------+------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter('Close<200').limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|count(Date)|\n",
      "+-----------+\n",
      "|       1762|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1762"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(count('Date')).show() # to take aggregation function result in a column\n",
    "\n",
    "df.select('Date').count() #same as df.count()\n",
    "# count() won't count NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year=df.withColumn('df_year',year('Date')) # extracting year from timestamp type column and adding a column containing extracted year in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|df_year|         max(High)|\n",
      "+-------+------------------+\n",
      "|   2016|        118.690002|\n",
      "|   2015|134.53999299999998|\n",
      "|   2014|        651.259979|\n",
      "|   2013|        575.139999|\n",
      "|   2012|        705.070023|\n",
      "|   2011|426.69999299999995|\n",
      "|   2010|            326.66|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_year.groupBy('df_year').max('High').orderBy(desc('df_year')).show()\n",
    "#df_year.groupBy('df_year').max('High').alias('max_high').orderBy('df_year',ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_year.groupBy('df_year').max('High')) #return type of aggregation function on group by is a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+--------------------+------------------+------------+\n",
      "|df_year|         avg(Open)|         avg(High)|          avg(Low)|        avg(Close)|         avg(Volume)|    avg(Adj Close)|avg(df_year)|\n",
      "+-------+------------------+------------------+------------------+------------------+--------------------+------------------+------------+\n",
      "|   2015|120.17575393253965|121.24452385714291| 118.8630954325397|120.03999980555547|  5.18378869047619E7|115.96740080555561|      2015.0|\n",
      "|   2013| 473.1281355634922| 477.6389272301587|468.24710264682557| 472.6348802857143|          1.016087E8| 62.61798788492063|      2013.0|\n",
      "|   2014| 295.1426195357143|297.56103184523823| 292.9949599801587| 295.4023416507935| 6.315273055555555E7| 87.63583323809523|      2014.0|\n",
      "|   2012|     576.652720788| 581.8254008040001| 569.9211606079999| 576.0497195640002|       1.319642044E8| 74.81383696800002|      2012.0|\n",
      "|   2016|104.50777772619044| 105.4271825436508|103.69027771825397|104.60400786904763|  3.84153623015873E7|103.15032854761901|      2016.0|\n",
      "|   2010| 259.9576190992064|262.36880881349214|256.84761791269847| 259.8424600000002|1.4982631666666666E8|33.665072424603196|      2010.0|\n",
      "|   2011|364.06142773412705| 367.4235704880951|360.29769878174613|364.00432532142867|1.2307474166666667E8| 47.16023692063492|      2011.0|\n",
      "+-------+------------------+------------------+------------------+------------------+--------------------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_year.groupBy(\"df_year\").mean().show() # mean on all columns in df_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        avg(Close)|\n",
      "+------------------+\n",
      "|120.03999980555547|\n",
      "| 472.6348802857143|\n",
      "| 295.4023416507935|\n",
      "| 576.0497195640002|\n",
      "|104.60400786904763|\n",
      "| 259.8424600000002|\n",
      "|364.00432532142867|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_year.groupBy(\"df_Year\").mean()[['avg(Close)']].show() # df_year.groupBy('df_year').mean() is a dataframe from which we can select columns (double sq brackets) to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|avg(df_year)|        avg(Close)|\n",
      "+------------+------------------+\n",
      "|      2015.0|120.03999980555547|\n",
      "|      2013.0| 472.6348802857143|\n",
      "|      2014.0| 295.4023416507935|\n",
      "|      2012.0| 576.0497195640002|\n",
      "|      2016.0|104.60400786904763|\n",
      "|      2010.0| 259.8424600000002|\n",
      "|      2011.0|364.00432532142867|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_year.groupBy(\"df_Year\").mean()[['avg(df_year)','avg(Close)']].show() # for more than one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('avg(df_year)', 'double'), ('avg(Close)', 'double')]"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_year.groupBy(\"df_Year\").mean()[['avg(df_year)','avg(Close)']].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|avg(df_year)|        avg(Close)|\n",
      "+------------+------------------+\n",
      "|      2015.0|120.03999980555547|\n",
      "|      2013.0| 472.6348802857143|\n",
      "|      2014.0| 295.4023416507935|\n",
      "|      2012.0| 576.0497195640002|\n",
      "|      2016.0|104.60400786904763|\n",
      "|      2010.0| 259.8424600000002|\n",
      "|      2011.0|364.00432532142867|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_year.groupBy(\"df_Year\").mean()[['avg(df_year)','avg(Close)']].show()) # show() returns NoneType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Year)=2015.0, avg(Close)=120.03999980555547),\n",
       " Row(avg(Year)=2013.0, avg(Close)=472.6348802857143),\n",
       " Row(avg(Year)=2014.0, avg(Close)=295.4023416507935),\n",
       " Row(avg(Year)=2012.0, avg(Close)=576.0497195640002),\n",
       " Row(avg(Year)=2016.0, avg(Close)=104.60400786904763),\n",
       " Row(avg(Year)=2010.0, avg(Close)=259.8424600000002),\n",
       " Row(avg(Year)=2011.0, avg(Close)=364.00432532142867)]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=df.withColumn(\"Year\",year(df['Date'])).groupBy(\"Year\").mean()[['avg(Year)','avg(Close)']].collect() # collect function returns list type\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(Year)=2015.0, avg(Close)=120.03999980555547),\n",
       " Row(avg(Year)=2013.0, avg(Close)=472.6348802857143),\n",
       " Row(avg(Year)=2014.0, avg(Close)=295.4023416507935),\n",
       " Row(avg(Year)=2012.0, avg(Close)=576.0497195640002),\n",
       " Row(avg(Year)=2016.0, avg(Close)=104.60400786904763)]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:5] # first 5 rows from list of row objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|Month|        avg(Close)|\n",
      "+-----+------------------+\n",
      "|    1|322.20971425714276|\n",
      "|    2| 321.3595563037038|\n",
      "|    3|332.91156731372547|\n",
      "|    4|340.51041081506827|\n",
      "|    5|351.62102085714304|\n",
      "|    6|      288.12546566|\n",
      "|    7|281.72216211486483|\n",
      "|    8| 300.4385809612901|\n",
      "|    9| 301.0763195902777|\n",
      "|   10|308.30552563157886|\n",
      "|   11| 306.2725174895104|\n",
      "|   12|302.35053626845644|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_month = df.withColumn('Month',month(df['Date']))\n",
    "df_month.groupBy('Month').avg('Close').orderBy('Month').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schema inference\n",
    "\n",
    "df3 = spark.read.json('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\people.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Programmatically specifying schema\n",
    "\n",
    "fieldlist=[StructField('age',IntegerType(),True),StructField('name',StringType(),True)] #list of StructFields for StructType\n",
    "schema=StructType(fieldlist)\n",
    "\n",
    "df4=spark.read.json('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\people.json',schema) #providing schema in json()\n",
    "df4.show()\n",
    "\n",
    "#df4=spark.read.format('json').load('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\people.json',schema=schema) # to provide schena in load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('age', 'int'), ('name', 'string')]"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.dtypes # to get (columnname,datatype) in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 19|Justin|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.filter(df4.age != 30).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "+----+-------+\n",
      "\n",
      "+----+\n",
      "| age|\n",
      "+----+\n",
      "|null|\n",
      "|  30|\n",
      "|  19|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4.filter(df4['age'].isNull()).show() #to check for NULL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "df4.storageLevel   # to get strage level of df (normal df (with no cache/persist is serialized with no saving in memory/disk)\n",
    "print(df4.storageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "df4.persist() # default MEMORY_DISK_ONLY (serialized with aving in memory & disk)\n",
    "print(df4.storageLevel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[age: int, name: string]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.unpersist()  # unpersist df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Memory Deserialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "df4.cache()  #df will be deserialized but will be saved in memory and disk by default\n",
    "print(df4.storageLevel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.is_cached # to check if cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disk Serialized 2x Replicated\n"
     ]
    }
   ],
   "source": [
    "df4.persist(pyspark.StorageLevel.DISK_ONLY_2)# to specify storage level\n",
    "print(df4.storageLevel) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 2, 3: 4}"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(1, 2), (3, 4)]).collectAsMap() # returns (k,v) element in a rdd as k:v (dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+-----------+------------------+\n",
      "| max(Open)| max(High)|  max(Low)|max(Close)|max(Volume)|    max(Adj Close)|\n",
      "+----------+----------+----------+----------+-----------+------------------+\n",
      "|702.409988|705.070023|699.569977|702.100021|  470249500|127.96609099999999|\n",
      "+----------+----------+----------+----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().max().show() # won't take max on date column as it is non numeric\n",
    "\n",
    "# max,min,mean,avg operates on numerical columns only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| max(Open)|\n",
      "+----------+\n",
      "|702.409988|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({'Open':'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-----------------+------------------+-----------+------------------+------------+\n",
      "|df_year|         max(Open)|         max(High)|         max(Low)|        max(Close)|max(Volume)|    max(Adj Close)|max(df_year)|\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------+------------------+------------+\n",
      "|   2015|134.46000700000002|134.53999299999998|       131.399994|             133.0|  162206300|127.96609099999999|        2015|\n",
      "|   2013|        572.650009|        575.139999|566.4100269999999|        570.090004|  365213100|         76.388451|        2013|\n",
      "|   2014|        649.900002|        651.259979|       644.470024|        647.349983|  266380800|113.96566299999999|        2014|\n",
      "|   2012|        702.409988|        705.070023|       699.569977|        702.100021|  376530000| 91.35431700000001|        2012|\n",
      "|   2016|            118.18|        118.690002|       117.449997|            118.25|  133369700|117.13811799999999|        2016|\n",
      "|   2010|         326.21999|            326.66|       325.099991|        325.470013|  466777500|         42.167749|        2010|\n",
      "|   2011|        421.759987|426.69999299999995|       415.990002|422.23999800000007|  470249500|         54.705225|        2011|\n",
      "+-------+------------------+------------------+-----------------+------------------+-----------+------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_year.groupBy('df_year').max().show() # grouped column name will be in data frame along with aggregated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=sc.parallelize([[1,2],[1,2],[1,2],[3,4],[5,6]]).toDF(['int1','int2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|int1|int2|\n",
      "+----+----+\n",
      "|   1|   2|\n",
      "|   1|   2|\n",
      "|   1|   2|\n",
      "|   3|   4|\n",
      "|   5|   6|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|int2|\n",
      "+----+\n",
      "|   2|\n",
      "|   2|\n",
      "|   2|\n",
      "|   4|\n",
      "|   6|\n",
      "+----+\n",
      "\n",
      "+----+\n",
      "|int2|\n",
      "+----+\n",
      "|   2|\n",
      "|   2|\n",
      "|   2|\n",
      "|   4|\n",
      "|   6|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(col('int2')).show() # to get value of a column\n",
    "\n",
    "#df5.select('int2').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT int1)|\n",
      "+--------------------+\n",
      "|                   3|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(countDistinct('int1')).show() # to count distict values in column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|int1|int2|\n",
      "+----+----+\n",
      "|   5|   6|\n",
      "|   3|   4|\n",
      "|   1|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.distinct().show() # to drop duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|int1|int2|\n",
      "+----+----+\n",
      "|   5|   6|\n",
      "|   3|   4|\n",
      "|   1|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6=df5.dropDuplicates() # to drop duplicate rows\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|avg(int1)|\n",
      "+---------+\n",
      "|      2.2|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.select(mean('int1').alias('mean_int1')).show() # to find mean/avg of col\n",
    "\n",
    "#df5.select(avg('int1')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|int1|int2|\n",
      "+----+----+\n",
      "|   5|   6|\n",
      "|   3|   4|\n",
      "|   1|   2|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df5.orderBy(df5[\"int1\"].desc()).distinct().show() # descending order and then distinct\n",
    "#df5.orderBy(df5[\"int1\"],ascending=False).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "df7= spark.read.format('csv').option('header','True').option('inferSchema','True').csv(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\ContainsNull.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8=df7.dropna('any') # 'any' (default if not specified) means drop row if NULL any where in a row (df.dropna(how='any'/'all', thresh(threshold number of non NULL,drop if non nulls are less than thresh value=None(default), subset(specific column(s) to look for=None(default))\n",
    "#df8=df7.na.drop()\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9=df7.dropna('all')\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df10=df7.dropna(thresh=2).show() #min 2 non null values are needed in a row to not drop\n",
    "df10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n",
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.dropna(subset='Sales').show() # drop null in specific column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+\n",
      "|  Id|       Name|Sales|\n",
      "+----+-----------+-----+\n",
      "|emp1|       John| null|\n",
      "|emp2|fill_value1| null|\n",
      "|emp3|fill_value1|345.0|\n",
      "|emp4|      Cindy|456.0|\n",
      "+----+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to fill null values\n",
    "\n",
    "#df7.na.fill('fill_value').show() # since null fill value is a string so only string columns will be filled .Numeric columns will still be NULL\n",
    "\n",
    "#appropriate way\n",
    "df7.na.fill('fill_value1',subset='Name').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|  1.1|\n",
      "|emp2| null|  1.1|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to fill null values\n",
    "\n",
    "#df7.na.fill(0).show() # since null fill value is a numeric so only numeric columns will be filled .Non - Numeric columns will still be NULL\n",
    "\n",
    "#df7.na.fill('fill_value1',subset='Sales').show() # will not happen as fill value is string and column we are looking is a numeric column\n",
    "\n",
    "#appropriate way\n",
    "\n",
    "df7.na.fill(1.1,subset='Sales').show() \n",
    "\n",
    "#df7.na.fill(1.1,'Sales').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
