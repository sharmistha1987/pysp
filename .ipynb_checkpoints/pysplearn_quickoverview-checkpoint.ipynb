{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "       .appName(\"Pyspark_learn\")\\\n",
    "       .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tF = spark.read.text(r\"D:\\spark-2.3.2-bin-hadoop2.7\\README.md\") # create data frame from text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1543949774700'),\n",
       " ('spark.driver.port', '54316'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'Pyspark_learn'),\n",
       " ('spark.driver.host', 'DESKTOP-QN77D5V.lan'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1543949774700'),\n",
       " ('spark.driver.port', '54316'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'Pyspark_learn'),\n",
       " ('spark.driver.host', 'DESKTOP-QN77D5V.lan'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('spark.app.id', 'local-1543949774700')\n",
      "('spark.app.name', 'Pyspark_learn')\n",
      "('spark.driver.host', 'DESKTOP-QN77D5V.lan')\n",
      "('spark.driver.port', '54316')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.sql.catalogImplementation', 'hive')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "for i in sorted(spark.sparkContext.getConf().getAll()):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.SparkConf().getAll() # or setAll([])\n",
    "\n",
    "# if using spark(as sparksession) then use spark.sparkContext._conf.setAll([<set properties>])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tF.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='# Apache Spark')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tF.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='# Apache Spark'),\n",
       " Row(value=''),\n",
       " Row(value='Spark is a fast and general cluster computing system for Big Data. It provides'),\n",
       " Row(value='high-level APIs in Scala, Java, Python, and R, and an optimized engine that'),\n",
       " Row(value='supports general computation graphs for data analysis. It also supports a'),\n",
       " Row(value='rich set of higher-level tools including Spark SQL for SQL and DataFrames,'),\n",
       " Row(value='MLlib for machine learning, GraphX for graph processing,'),\n",
       " Row(value='and Spark Streaming for stream processing.'),\n",
       " Row(value=''),\n",
       " Row(value='<http://spark.apache.org/>'),\n",
       " Row(value=''),\n",
       " Row(value=''),\n",
       " Row(value='## Online Documentation'),\n",
       " Row(value=''),\n",
       " Row(value='You can find the latest Spark documentation, including a programming'),\n",
       " Row(value='guide, on the [project web page](http://spark.apache.org/documentation.html).'),\n",
       " Row(value='This README file only contains basic setup instructions.'),\n",
       " Row(value=''),\n",
       " Row(value='## Building Spark'),\n",
       " Row(value=''),\n",
       " Row(value='Spark is built using [Apache Maven](http://maven.apache.org/).'),\n",
       " Row(value='To build Spark and its example programs, run:'),\n",
       " Row(value=''),\n",
       " Row(value='    build/mvn -DskipTests clean package'),\n",
       " Row(value=''),\n",
       " Row(value='(You do not need to do this if you downloaded a pre-built package.)'),\n",
       " Row(value=''),\n",
       " Row(value='You can build Spark using more than one thread by using the -T option with Maven, see [\"Parallel builds in Maven 3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).'),\n",
       " Row(value='More detailed documentation is available from the project site, at'),\n",
       " Row(value='[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).'),\n",
       " Row(value=''),\n",
       " Row(value='For general development tips, including info on developing Spark using an IDE, see [\"Useful Developer Tools\"](http://spark.apache.org/developer-tools.html).'),\n",
       " Row(value=''),\n",
       " Row(value='## Interactive Scala Shell'),\n",
       " Row(value=''),\n",
       " Row(value='The easiest way to start using Spark is through the Scala shell:'),\n",
       " Row(value=''),\n",
       " Row(value='    ./bin/spark-shell'),\n",
       " Row(value=''),\n",
       " Row(value='Try the following command, which should return 1000:'),\n",
       " Row(value=''),\n",
       " Row(value='    scala> sc.parallelize(1 to 1000).count()'),\n",
       " Row(value=''),\n",
       " Row(value='## Interactive Python Shell'),\n",
       " Row(value=''),\n",
       " Row(value='Alternatively, if you prefer Python, you can use the Python shell:'),\n",
       " Row(value=''),\n",
       " Row(value='    ./bin/pyspark'),\n",
       " Row(value=''),\n",
       " Row(value='And run the following command, which should also return 1000:'),\n",
       " Row(value=''),\n",
       " Row(value='    >>> sc.parallelize(range(1000)).count()'),\n",
       " Row(value=''),\n",
       " Row(value='## Example Programs'),\n",
       " Row(value=''),\n",
       " Row(value='Spark also comes with several sample programs in the `examples` directory.'),\n",
       " Row(value='To run one of them, use `./bin/run-example <class> [params]`. For example:'),\n",
       " Row(value=''),\n",
       " Row(value='    ./bin/run-example SparkPi'),\n",
       " Row(value=''),\n",
       " Row(value='will run the Pi example locally.'),\n",
       " Row(value=''),\n",
       " Row(value='You can set the MASTER environment variable when running examples to submit'),\n",
       " Row(value='examples to a cluster. This can be a mesos:// or spark:// URL,'),\n",
       " Row(value='\"yarn\" to run on YARN, and \"local\" to run'),\n",
       " Row(value='locally with one thread, or \"local[N]\" to run locally with N threads. You'),\n",
       " Row(value='can also use an abbreviated class name if the class is in the `examples`'),\n",
       " Row(value='package. For instance:'),\n",
       " Row(value=''),\n",
       " Row(value='    MASTER=spark://host:7077 ./bin/run-example SparkPi'),\n",
       " Row(value=''),\n",
       " Row(value='Many of the example programs print usage help if no params are given.'),\n",
       " Row(value=''),\n",
       " Row(value='## Running Tests'),\n",
       " Row(value=''),\n",
       " Row(value='Testing first requires [building Spark](#building-spark). Once Spark is built, tests'),\n",
       " Row(value='can be run using:'),\n",
       " Row(value=''),\n",
       " Row(value='    ./dev/run-tests'),\n",
       " Row(value=''),\n",
       " Row(value='Please see the guidance on how to'),\n",
       " Row(value='[run tests for a module, or individual tests](http://spark.apache.org/developer-tools.html#individual-tests).'),\n",
       " Row(value=''),\n",
       " Row(value='## A Note About Hadoop Versions'),\n",
       " Row(value=''),\n",
       " Row(value='Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported'),\n",
       " Row(value='storage systems. Because the protocols have changed in different versions of'),\n",
       " Row(value='Hadoop, you must build Spark against the same version that your cluster runs.'),\n",
       " Row(value=''),\n",
       " Row(value='Please refer to the build documentation at'),\n",
       " Row(value='[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)'),\n",
       " Row(value='for detailed guidance on building for a particular distribution of Hadoop, including'),\n",
       " Row(value='building for particular Hive and Hive Thriftserver distributions.'),\n",
       " Row(value=''),\n",
       " Row(value='## Configuration'),\n",
       " Row(value=''),\n",
       " Row(value='Please refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)'),\n",
       " Row(value='in the online documentation for an overview on how to configure Spark.'),\n",
       " Row(value=''),\n",
       " Row(value='## Contributing'),\n",
       " Row(value=''),\n",
       " Row(value='Please review the [Contribution to Spark guide](http://spark.apache.org/contributing.html)'),\n",
       " Row(value='for information on how to get started contributing to the project.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='# Apache Spark'),\n",
       " Row(value=''),\n",
       " Row(value='Spark is a fast and general cluster computing system for Big Data. It provides'),\n",
       " Row(value='high-level APIs in Scala, Java, Python, and R, and an optimized engine that'),\n",
       " Row(value='supports general computation graphs for data analysis. It also supports a')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|      # Apache Spark|\n",
      "|                    |\n",
      "|Spark is a fast a...|\n",
      "|high-level APIs i...|\n",
      "|supports general ...|\n",
      "|rich set of highe...|\n",
      "|MLlib for machine...|\n",
      "|and Spark Streami...|\n",
      "|                    |\n",
      "|<http://spark.apa...|\n",
      "|                    |\n",
      "|                    |\n",
      "|## Online Documen...|\n",
      "|                    |\n",
      "|You can find the ...|\n",
      "|guide, on the [pr...|\n",
      "|This README file ...|\n",
      "|                    |\n",
      "|   ## Building Spark|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|      # Apache Spark|\n",
      "|Spark is a fast a...|\n",
      "|rich set of highe...|\n",
      "|and Spark Streami...|\n",
      "|You can find the ...|\n",
      "|   ## Building Spark|\n",
      "|Spark is built us...|\n",
      "|To build Spark an...|\n",
      "|You can build Spa...|\n",
      "|[\"Building Spark\"...|\n",
      "|For general devel...|\n",
      "|The easiest way t...|\n",
      "|Spark also comes ...|\n",
      "|    ./bin/run-exa...|\n",
      "|    MASTER=spark:...|\n",
      "|Testing first req...|\n",
      "|Spark uses the Ha...|\n",
      "|Hadoop, you must ...|\n",
      "|in the online doc...|\n",
      "|Please review the...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=tF.filter(tF.value.contains(\"Spark\")) #<dataframe>.value.contains() to access contents of column\n",
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(numWords)=['will', 'run', 'the', 'Pi', 'example', 'locally.'])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tF.select(split(tF.value, \"\\s+\").name(\"numWords\")).agg(max(col(\"numWords\"))).collect() # name()  is used to create alias of computed column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(numWords)=22)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tF.select(size(split(tF.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(word='online', count=1),\n",
       " Row(word='graphs', count=1),\n",
       " Row(word='[\"Parallel', count=1),\n",
       " Row(word='[\"Building', count=1),\n",
       " Row(word='thread', count=1),\n",
       " Row(word='documentation', count=3),\n",
       " Row(word='command,', count=2),\n",
       " Row(word='abbreviated', count=1),\n",
       " Row(word='overview', count=1),\n",
       " Row(word='rich', count=1),\n",
       " Row(word='set', count=2),\n",
       " Row(word='-DskipTests', count=1),\n",
       " Row(word='name', count=1),\n",
       " Row(word='page](http://spark.apache.org/documentation.html).', count=1),\n",
       " Row(word='[\"Specifying', count=1),\n",
       " Row(word='stream', count=1),\n",
       " Row(word='run:', count=1),\n",
       " Row(word='not', count=1),\n",
       " Row(word='programs', count=2),\n",
       " Row(word='tests', count=2),\n",
       " Row(word='./dev/run-tests', count=1),\n",
       " Row(word='will', count=1),\n",
       " Row(word='[run', count=1),\n",
       " Row(word='particular', count=2),\n",
       " Row(word='option', count=1),\n",
       " Row(word='Alternatively,', count=1),\n",
       " Row(word='by', count=1),\n",
       " Row(word='must', count=1),\n",
       " Row(word='using', count=5),\n",
       " Row(word='you', count=4),\n",
       " Row(word='MLlib', count=1),\n",
       " Row(word='DataFrames,', count=1),\n",
       " Row(word='variable', count=1),\n",
       " Row(word='Note', count=1),\n",
       " Row(word='core', count=1),\n",
       " Row(word='more', count=1),\n",
       " Row(word='protocols', count=1),\n",
       " Row(word='guidance', count=2),\n",
       " Row(word='shell:', count=2),\n",
       " Row(word='can', count=7),\n",
       " Row(word='site,', count=1),\n",
       " Row(word='systems.', count=1),\n",
       " Row(word='Maven', count=1),\n",
       " Row(word='[building', count=1),\n",
       " Row(word='configure', count=1),\n",
       " Row(word='for', count=12),\n",
       " Row(word='README', count=1),\n",
       " Row(word='Interactive', count=2),\n",
       " Row(word='how', count=3),\n",
       " Row(word='[Configuration', count=1),\n",
       " Row(word='Hive', count=2),\n",
       " Row(word='system', count=1),\n",
       " Row(word='provides', count=1),\n",
       " Row(word='Hadoop-supported', count=1),\n",
       " Row(word='pre-built', count=1),\n",
       " Row(word='[\"Useful', count=1),\n",
       " Row(word='directory.', count=1),\n",
       " Row(word='Example', count=1),\n",
       " Row(word='example', count=3),\n",
       " Row(word='one', count=3),\n",
       " Row(word='MASTER', count=1),\n",
       " Row(word='in', count=6),\n",
       " Row(word='library', count=1),\n",
       " Row(word='Spark.', count=1),\n",
       " Row(word='contains', count=1),\n",
       " Row(word='Configuration', count=1),\n",
       " Row(word='programming', count=1),\n",
       " Row(word='with', count=4),\n",
       " Row(word='contributing', count=1),\n",
       " Row(word='downloaded', count=1),\n",
       " Row(word='1000).count()', count=1),\n",
       " Row(word='comes', count=1),\n",
       " Row(word='machine', count=1),\n",
       " Row(word='Tools\"](http://spark.apache.org/developer-tools.html).', count=1),\n",
       " Row(word='Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)', count=1),\n",
       " Row(word='building', count=2),\n",
       " Row(word='params', count=1),\n",
       " Row(word='Guide](http://spark.apache.org/docs/latest/configuration.html)', count=1),\n",
       " Row(word='given.', count=1),\n",
       " Row(word='be', count=2),\n",
       " Row(word='same', count=1),\n",
       " Row(word='than', count=1),\n",
       " Row(word='Programs', count=1),\n",
       " Row(word='locally', count=2),\n",
       " Row(word='using:', count=1),\n",
       " Row(word='fast', count=1),\n",
       " Row(word='[Apache', count=1),\n",
       " Row(word='your', count=1),\n",
       " Row(word='optimized', count=1),\n",
       " Row(word='Developer', count=1),\n",
       " Row(word='R,', count=1),\n",
       " Row(word='should', count=2),\n",
       " Row(word='graph', count=1),\n",
       " Row(word='package', count=1),\n",
       " Row(word='-T', count=1),\n",
       " Row(word='[project', count=1),\n",
       " Row(word='project', count=1),\n",
       " Row(word='`examples`', count=2),\n",
       " Row(word='versions', count=1),\n",
       " Row(word='Spark](#building-spark).', count=1),\n",
       " Row(word='general', count=3),\n",
       " Row(word='other', count=1),\n",
       " Row(word='learning,', count=1),\n",
       " Row(word='when', count=1),\n",
       " Row(word='submit', count=1),\n",
       " Row(word='Apache', count=1),\n",
       " Row(word='1000:', count=2),\n",
       " Row(word='detailed', count=2),\n",
       " Row(word='About', count=1),\n",
       " Row(word='is', count=6),\n",
       " Row(word='on', count=7),\n",
       " Row(word='scala>', count=1),\n",
       " Row(word='print', count=1),\n",
       " Row(word='use', count=3),\n",
       " Row(word='different', count=1),\n",
       " Row(word='following', count=2),\n",
       " Row(word='SparkPi', count=2),\n",
       " Row(word='refer', count=2),\n",
       " Row(word='./bin/run-example', count=2),\n",
       " Row(word='data', count=1),\n",
       " Row(word='Tests', count=1),\n",
       " Row(word='Versions', count=1),\n",
       " Row(word='Data.', count=1),\n",
       " Row(word='processing.', count=1),\n",
       " Row(word='its', count=1),\n",
       " Row(word='basic', count=1),\n",
       " Row(word='latest', count=1),\n",
       " Row(word='only', count=1),\n",
       " Row(word='<class>', count=1),\n",
       " Row(word='have', count=1),\n",
       " Row(word='runs.', count=1),\n",
       " Row(word='You', count=4),\n",
       " Row(word='tips,', count=1),\n",
       " Row(word='project.', count=1),\n",
       " Row(word='developing', count=1),\n",
       " Row(word='YARN,', count=1),\n",
       " Row(word='It', count=2),\n",
       " Row(word='\"local\"', count=1),\n",
       " Row(word='processing,', count=1),\n",
       " Row(word='built', count=1),\n",
       " Row(word='Pi', count=1),\n",
       " Row(word='thread,', count=1),\n",
       " Row(word='A', count=1),\n",
       " Row(word='APIs', count=1),\n",
       " Row(word='Scala,', count=1),\n",
       " Row(word='file', count=1),\n",
       " Row(word='computation', count=1),\n",
       " Row(word='Once', count=1),\n",
       " Row(word='find', count=1),\n",
       " Row(word='the', count=24),\n",
       " Row(word='To', count=2),\n",
       " Row(word='sc.parallelize(1', count=1),\n",
       " Row(word='uses', count=1),\n",
       " Row(word='N', count=1),\n",
       " Row(word='programs,', count=1),\n",
       " Row(word='\"yarn\"', count=1),\n",
       " Row(word='see', count=3),\n",
       " Row(word='./bin/pyspark', count=1),\n",
       " Row(word='return', count=2),\n",
       " Row(word='computing', count=1),\n",
       " Row(word='Java,', count=1),\n",
       " Row(word='from', count=1),\n",
       " Row(word='Because', count=1),\n",
       " Row(word='cluster', count=2),\n",
       " Row(word='Streaming', count=1),\n",
       " Row(word='More', count=1),\n",
       " Row(word='analysis.', count=1),\n",
       " Row(word='Maven](http://maven.apache.org/).', count=1),\n",
       " Row(word='cluster.', count=1),\n",
       " Row(word='Running', count=1),\n",
       " Row(word='Please', count=4),\n",
       " Row(word='talk', count=1),\n",
       " Row(word='distributions.', count=1),\n",
       " Row(word='guide,', count=1),\n",
       " Row(word='tests](http://spark.apache.org/developer-tools.html#individual-tests).', count=1),\n",
       " Row(word='\"local[N]\"', count=1),\n",
       " Row(word='Try', count=1),\n",
       " Row(word='and', count=9),\n",
       " Row(word='do', count=2),\n",
       " Row(word='Scala', count=2),\n",
       " Row(word='class', count=2),\n",
       " Row(word='build', count=4),\n",
       " Row(word='3\"](https://cwiki.apache.org/confluence/display/MAVEN/Parallel+builds+in+Maven+3).', count=1),\n",
       " Row(word='setup', count=1),\n",
       " Row(word='need', count=1),\n",
       " Row(word='spark://', count=1),\n",
       " Row(word='Hadoop,', count=2),\n",
       " Row(word='Thriftserver', count=1),\n",
       " Row(word='are', count=1),\n",
       " Row(word='requires', count=1),\n",
       " Row(word='package.', count=1),\n",
       " Row(word='clean', count=1),\n",
       " Row(word='sc.parallelize(range(1000)).count()', count=1),\n",
       " Row(word='high-level', count=1),\n",
       " Row(word='SQL', count=2),\n",
       " Row(word='against', count=1),\n",
       " Row(word='of', count=5),\n",
       " Row(word='through', count=1),\n",
       " Row(word='review', count=1),\n",
       " Row(word='package.)', count=1),\n",
       " Row(word='Python,', count=2),\n",
       " Row(word='easiest', count=1),\n",
       " Row(word='no', count=1),\n",
       " Row(word='Testing', count=1),\n",
       " Row(word='several', count=1),\n",
       " Row(word='help', count=1),\n",
       " Row(word='The', count=1),\n",
       " Row(word='sample', count=1),\n",
       " Row(word='MASTER=spark://host:7077', count=1),\n",
       " Row(word='Big', count=1),\n",
       " Row(word='examples', count=2),\n",
       " Row(word='an', count=4),\n",
       " Row(word='#', count=1),\n",
       " Row(word='Online', count=1),\n",
       " Row(word='including', count=4),\n",
       " Row(word='usage', count=1),\n",
       " Row(word='Python', count=2),\n",
       " Row(word='at', count=2),\n",
       " Row(word='development', count=1),\n",
       " Row(word='Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', count=1),\n",
       " Row(word='IDE,', count=1),\n",
       " Row(word='way', count=1),\n",
       " Row(word='Contributing', count=1),\n",
       " Row(word='get', count=1),\n",
       " Row(word='that', count=2),\n",
       " Row(word='##', count=9),\n",
       " Row(word='For', count=3),\n",
       " Row(word='prefer', count=1),\n",
       " Row(word='This', count=2),\n",
       " Row(word='build/mvn', count=1),\n",
       " Row(word='builds', count=1),\n",
       " Row(word='running', count=1),\n",
       " Row(word='web', count=1),\n",
       " Row(word='run', count=7),\n",
       " Row(word='locally.', count=1),\n",
       " Row(word='Spark', count=16),\n",
       " Row(word='URL,', count=1),\n",
       " Row(word='a', count=8),\n",
       " Row(word='higher-level', count=1),\n",
       " Row(word='tools', count=1),\n",
       " Row(word='if', count=4),\n",
       " Row(word='available', count=1),\n",
       " Row(word='', count=47),\n",
       " Row(word='Documentation', count=1),\n",
       " Row(word='this', count=1),\n",
       " Row(word='(You', count=1),\n",
       " Row(word='>>>', count=1),\n",
       " Row(word='information', count=1),\n",
       " Row(word='info', count=1),\n",
       " Row(word='<http://spark.apache.org/>', count=1),\n",
       " Row(word='Shell', count=2),\n",
       " Row(word='environment', count=1),\n",
       " Row(word='built,', count=1),\n",
       " Row(word='module,', count=1),\n",
       " Row(word='them,', count=1),\n",
       " Row(word='`./bin/run-example', count=1),\n",
       " Row(word='instance:', count=1),\n",
       " Row(word='first', count=1),\n",
       " Row(word='[Contribution', count=1),\n",
       " Row(word='guide](http://spark.apache.org/contributing.html)', count=1),\n",
       " Row(word='documentation,', count=1),\n",
       " Row(word='[params]`.', count=1),\n",
       " Row(word='mesos://', count=1),\n",
       " Row(word='engine', count=1),\n",
       " Row(word='GraphX', count=1),\n",
       " Row(word='Maven,', count=1),\n",
       " Row(word='example:', count=1),\n",
       " Row(word='HDFS', count=1),\n",
       " Row(word='or', count=3),\n",
       " Row(word='to', count=17),\n",
       " Row(word='Hadoop', count=3),\n",
       " Row(word='individual', count=1),\n",
       " Row(word='also', count=4),\n",
       " Row(word='changed', count=1),\n",
       " Row(word='started', count=1),\n",
       " Row(word='./bin/spark-shell', count=1),\n",
       " Row(word='threads.', count=1),\n",
       " Row(word='supports', count=2),\n",
       " Row(word='storage', count=1),\n",
       " Row(word='version', count=1),\n",
       " Row(word='instructions.', count=1),\n",
       " Row(word='Building', count=1),\n",
       " Row(word='start', count=1),\n",
       " Row(word='Many', count=1),\n",
       " Row(word='which', count=2),\n",
       " Row(word='And', count=1),\n",
       " Row(word='distribution', count=1)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tF.select(explode(split(tF.value, \"\\s+\")).alias(\"word\")).groupBy(\"word\").count().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
