{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark=SparkSession.builder.appName('exer_spark').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Average Friends by Age ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0,Will,33,385',\n",
       " '1,Jean-Luc,26,2',\n",
       " '2,Hugh,55,221',\n",
       " '3,Deanna,40,465',\n",
       " '4,Quark,68,21']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_data=sc.textFile('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\fakefriends.csv')\n",
    "f_data.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_numf=f_data.map(lambda x:(x.split(',')[2],x.split(',')[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('33', ('385', 1)),\n",
       " ('26', ('2', 1)),\n",
       " ('55', ('221', 1)),\n",
       " ('40', ('465', 1)),\n",
       " ('68', ('21', 1))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_num_map=age_numf.mapValues(lambda x: (x,1))\n",
    "age_num_map.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('33', ('38574471275245356460294243463228410', 12)),\n",
       " ('26', ('228184282381145345293298492269254738312439184', 17)),\n",
       " ('40', ('4652544594071828438934940619817233567261286220', 17)),\n",
       " ('68', ('21264112490481217189206293423', 10)),\n",
       " ('54', ('30725375440744123536939746272442115', 13))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_totalnum=age_num_map.reduceByKey(lambda x,y: (x[0] + y[0], x[1] + y[1]))\n",
    "age_totalnum.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('33', 3.214539272937113e+33),\n",
       " ('26', 1.3422604845949725e+43),\n",
       " ('40', 2.736790937689311e+44),\n",
       " ('68', 2.1264112490481217e+27),\n",
       " ('54', 2.3634904185187785e+33)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_age=age_totalnum.mapValues(lambda x:float(x[0])/x[1])\n",
    "\n",
    "average_age.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordcount of a text file with regex ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-Employment:',\n",
       " 'Building',\n",
       " 'an',\n",
       " 'Internet',\n",
       " 'Business',\n",
       " 'of',\n",
       " 'One',\n",
       " 'Achieving',\n",
       " 'Financial',\n",
       " 'and']"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines=sc.textFile('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\Book.txt')\n",
    "#words=lines.flatMap(lambda x:x.split())\n",
    "#words.take(10) # there are many special characters and blanks (seen thru collect()),so we need to use regex to ensure only valid words are selected\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def regex_pr(line): # to filter special characters\n",
    "    c=re.compile(r'\\W+',re.UNICODE)\n",
    "    return c.split(line.lower())  # after compiling regexp for detecting valid chars apply that to \n",
    "                                  # split line where capitals and smalls are treated equal. \n",
    "    #return re.split(c,line.lower())\n",
    "    #re.split(<pattern,<text>) expects a text s argument.\n",
    "    #re.compile(<pattern>,<flag>) and saving the resulting regular expression object for reuse is more efficient\n",
    "    #when the expression will be used several times in a single program\n",
    "    # re.UNICODE to specify the text has some unicode info . you can also write it as re.U.\n",
    " \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['self',\n",
       " 'employment',\n",
       " 'building',\n",
       " 'an',\n",
       " 'internet',\n",
       " 'business',\n",
       " 'of',\n",
       " 'one',\n",
       " 'achieving',\n",
       " 'financial']"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=lines.flatMap(regex_pr)  \n",
    "words.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('self', 111), ('an', 178), ('internet', 26), ('business', 383), ('of', 970)]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_of_words=words.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)\n",
    "count_of_words.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('self', 111)\n",
      "('an', 178)\n",
      "('internet', 26)\n",
      "('business', 383)\n",
      "('of', 970)\n"
     ]
    }
   ],
   "source": [
    "for c in count_of_words.take(5): # printing count of words\n",
    "    print(c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1878, 'you')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_by_count=count_of_words.map(lambda x :(x[1],x[0])).sortByKey(False)\n",
    "sort_by_count.first() # max count and related word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self:111\n",
      "an:178\n",
      "internet:26\n",
      "business:383\n",
      "of:970\n"
     ]
    }
   ],
   "source": [
    "for c in count_of_words.take(5): # formating result as word:count\n",
    "    count = str(c[1])\n",
    "    word = str(c[0])\n",
    "    print(word+':'+count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the Most Popular Superhero and coappearances in a Social Graph ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseNames(line):\n",
    "    fields = line.split('\\\"') #split on quotes to get key and name\n",
    "    return (int(fields[0]), fields[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countCoOccurences(line):\n",
    "    #elements = line.strip().split()\n",
    "    elements = line.split()\n",
    "    return (int(elements[0]), len(elements) -1) # to subtract id (key) from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 \"24-HOUR MAN/EMMANUEL\"',\n",
       " '2 \"3-D MAN/CHARLES CHAN\"',\n",
       " '3 \"4-D MAN/MERCURIO\"',\n",
       " '4 \"8-BALL/\"',\n",
       " '5 \"A\"']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\marvel-names.txt\")\n",
    "names.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '24-HOUR MAN/EMMANUEL'),\n",
       " (2, '3-D MAN/CHARLES CHAN'),\n",
       " (3, '4-D MAN/MERCURIO'),\n",
       " (4, '8-BALL/'),\n",
       " (5, 'A'),\n",
       " (6, \"A'YIN\"),\n",
       " (7, 'ABBOTT, JACK'),\n",
       " (8, 'ABCISSA'),\n",
       " (9, 'ABEL'),\n",
       " (10, 'ABOMINATION/EMIL BLO')]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namesRdd = names.map(parseNames)\n",
    "namesRdd.take(10) # contains key of superhero,name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5988 748 1722 3752 4655 5743 1872 3413 5527 6368 6085 4319 4728 1636 2397 3364 4001 1614 1819 1585 732 2660 3952 2507 3891 2070 2239 2602 612 1352 5447 4548 1596 5488 1605 5517 11 479 2554 2043 17 865 4292 6312 473 534 1479 6375 4456 ',\n",
       " '5989 4080 4264 4446 3779 2430 2297 6169 3530 3272 4282 6432 2548 4140 185 105 3878 2429 1334 4595 2767 3956 3877 4776 4946 3407 128 269 5775 5121 481 5516 4758 4053 1044 1602 3889 1535 6038 533 3986 ',\n",
       " '5982 217 595 1194 3308 2940 1815 794 1503 5197 859 5096 6039 2664 651 2244 528 284 1449 1097 1172 1092 108 3405 5204 387 4607 4545 3705 4930 1805 4712 4404 247 4754 4427 1845 536 5795 5978 533 3984 6056 ',\n",
       " '5983 1165 3836 4361 1282 716 4289 4646 6300 5084 2397 4454 1913 5861 5485 ',\n",
       " '5980 2731 3712 1587 6084 2472 2546 6313 875 859 323 2664 1469 522 2506 2919 2423 3624 5736 5046 1787 5776 3245 3840 2399 ']"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\marvel-graph.txt\")\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5988, 48),\n",
       " (5989, 40),\n",
       " (5982, 42),\n",
       " (5983, 14),\n",
       " (5980, 24),\n",
       " (5981, 17),\n",
       " (5986, 142),\n",
       " (5987, 81),\n",
       " (5984, 41),\n",
       " (5985, 19)]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairings = lines.map(countCoOccurences)\n",
    "pairings.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(48, 5988),\n",
       " (42, 5982),\n",
       " (24, 5980),\n",
       " (142, 5986),\n",
       " (41, 5984),\n",
       " (13, 6294),\n",
       " (42, 270),\n",
       " (45, 272),\n",
       " (410, 274),\n",
       " (15, 276)]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalFriendsByCharacter = pairings.reduceByKey(lambda x, y : x + y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(859, 1933)"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostpopular=totalFriendsByCharacter.sortBy(lambda x:x[1],False).first() #sort value for each key in descending order\n",
    "mostpopular #key of superhero,occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1933, 859)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternate way of finding max value of key is using max()\n",
    "\n",
    "flipped = totalFriendsByCharacter.map(lambda x : (x[1],x[0]))\n",
    "flipped.take(10)\n",
    "mostpopular1=flipped.max() # max number of coappearances ,max() operate on key of k,v pair or max(<key function>) where key can be changed\n",
    "mostpopular1 #contains count of friends,key of superhero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popular name is CAPTAIN AMERICA with coappearances=1933\n"
     ]
    }
   ],
   "source": [
    "# to get matching name from id (key\n",
    "\n",
    "#for name in namesRdd.collect():\n",
    "    #if mostpopular1[1]==name[0]: # mostpopular[0]==name[0]:\n",
    "       #print(name[1])\n",
    "        \n",
    "#alternate way \n",
    "\n",
    "print('popular name is '+namesRdd.lookup(mostpopular[0])[0]+' with coappearances='+ str(mostpopular[1])) # lookup(<value>),to lookup for a value in rdd. Return list of matches if value found\n",
    "                                                                                                         #[0] after lookup() - to display value of list\n",
    "\n",
    "#print('popular name is '+namesRdd.lookup(mostpopular1[1])[0]+' with coappearances='+ str(mostpopular1[0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total-Amount-By-Customer##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\customer-orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['44,8602,37.19',\n",
       " '35,5368,65.89',\n",
       " '2,3391,40.64',\n",
       " '47,6694,14.98',\n",
       " '29,680,13.08']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    customerId = int(fields[0])\n",
    "    itemId = fields[1]\n",
    "    itemAmt = float(fields[2])\n",
    "    return (customerId, itemAmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 4756.890000000001),\n",
       " (2, 5994.59),\n",
       " (70, 5368.249999999999),\n",
       " (14, 4735.030000000001),\n",
       " (42, 5696.840000000002)]"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#orders = cust.map(parseLine)\n",
    "id_amt=cust.map(lambda line: (int(line.split(',')[0]),float(line.split(',')[2])))\n",
    "totals = id_amt.reduceByKey(lambda x, y: x + y)\n",
    "totals.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4756.89 \t44.00\n",
      "5994.59 \t2.00\n",
      "5368.25 \t70.00\n",
      "4735.03 \t14.00\n",
      "5696.84 \t42.00\n"
     ]
    }
   ],
   "source": [
    "for result in totals.take(5):\n",
    "    print(\"{:.2f}\".format(result[1]),\"\\t{:.2f}\".format(result[0])) #:.2f for formatting in placeholder and then using format() to display result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Min/Max -Temperatures per station ID## <same logic formax,below is only for min>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ITE00100554,18000101,TMAX,-75,,,E,',\n",
       " 'ITE00100554,18000101,TMIN,-148,,,E,',\n",
       " 'GM000010962,18000101,PRCP,0,,,E,',\n",
       " 'EZE00100082,18000101,TMAX,-86,,,E,',\n",
       " 'EZE00100082,18000101,TMIN,-135,,,E,']"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\1800.csv\")\n",
    "lines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To take off required fields and assigning them to variable,create function where split line on a pattern and assign variables to reqd fields.\n",
    "\n",
    "def parseLine(line):\n",
    "    fields = line.split(',')\n",
    "    stationID = fields[0]\n",
    "    entryType = fields[2]\n",
    "    temperature = float(fields[3]) * 0.1 * (9.0 / 5.0) + 32.0\n",
    "    return (stationID, entryType, temperature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 'TMAX', 18.5),\n",
       " ('ITE00100554', 'TMIN', 5.359999999999999),\n",
       " ('GM000010962', 'PRCP', 32.0),\n",
       " ('EZE00100082', 'TMAX', 16.52),\n",
       " ('EZE00100082', 'TMIN', 7.699999999999999)]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsedLines=lines.map(parseLine) # use the function to actual line element of rdd\n",
    "parsedLines.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 'TMIN', 5.359999999999999),\n",
       " ('EZE00100082', 'TMIN', 7.699999999999999),\n",
       " ('ITE00100554', 'TMIN', 9.5),\n",
       " ('EZE00100082', 'TMIN', 8.599999999999998),\n",
       " ('ITE00100554', 'TMIN', 23.72)]"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTemps = parsedLines.filter(lambda x: \"TMIN\" in x[1]) # check if TMIN as a 1st pos value in each element of rdd,hence use filter() to filter out such elements of rdd\n",
    "minTemps.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 5.359999999999999),\n",
       " ('EZE00100082', 7.699999999999999),\n",
       " ('ITE00100554', 9.5),\n",
       " ('EZE00100082', 8.599999999999998),\n",
       " ('ITE00100554', 23.72)]"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_temp=minTemps.map(lambda x:(x[0],x[2]))\n",
    "id_temp.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ITE00100554', 5.359999999999999), ('EZE00100082', 7.699999999999999)]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minTemps = id_temp.reduceByKey(lambda x,y: x if x<y else y) # to calculate min of values (temps) per key\n",
    "minTemps.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "station Id ITE00100554 has min temp of 5.36F\n",
      "station Id EZE00100082 has min temp of 7.70F\n",
      "\n",
      "station Id ITE00100554 has lowest temp of 5.36F of all stations\n"
     ]
    }
   ],
   "source": [
    "for m in minTemps.collect(): #display min temperature per station\n",
    "    print('station Id '+m[0]+' has min temp of {:.2f}F'.format(m[1]))\n",
    "    \n",
    "#to get lowest temperature and related station ID and of all stations.\n",
    "\n",
    "m=minTemps.min(lambda x:x[1]) # to get min temperature pair\n",
    "\n",
    "print('\\nstation Id '+m[0]+' has lowest temp of {:.2f}F '.format(m[1])+'of all stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some more spark opertions on a csv file ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoData = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\auto-data.csv\")\n",
    "autoData.cache() #action (to uncache use autoData.unpersist())\n",
    "autoData.is_cached #to check id rdd is cached #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAKE,FUELTYPE,ASPIRE,DOORS,BODY,DRIVE,CYLINDERS,HP,RPM,MPG-CITY,MPG-HWY,PRICE',\n",
       " 'subaru,gas,std,two,hatchback,fwd,four,69,4900,31,36,5118',\n",
       " 'chevrolet,gas,std,two,hatchback,fwd,three,48,5100,47,53,5151',\n",
       " 'mazda,gas,std,two,hatchback,fwd,four,68,5000,30,31,5195',\n",
       " 'toyota,gas,std,two,hatchback,fwd,four,62,4800,35,39,5348']"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoData.take(5) #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoData.count() #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MAKE,FUELTYPE,ASPIRE,DOORS,BODY,DRIVE,CYLINDERS,HP,RPM,MPG-CITY,MPG-HWY,PRICE'"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoData.first() #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoData.saveAsTextFile('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\auto-data-saved') # this will create a folder (auto-data-saved) auto-data-saved with multiple part text files\n",
    "\n",
    "autoData.coalesce(1).saveAsTextFile('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\auto-data-saved.csv') # saveAsTextFile(<ouputfolder>)\n",
    "\n",
    "#to avoid multiple part test files being created instead 1 part text file in auto-data-saved folder,\n",
    "#use coalesce(<num of partitions>) to combine parttions together in specified num of partitions.\n",
    "# Do not repartition() as that will cause shuffle hence expensive\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving rdd to file without creating folder and then file\n",
    "\n",
    "f=open(\"D:\\\\auto-data-saved.csv\",\"w\") # create new empty file in write mode\n",
    "\n",
    "f.write(\"\\n\".join(autoData.collect())) # <str1>.join(<iterable>) will join str1 with each element of iterable.In this case \\n to move each element of list to new row\n",
    "#autoDatacollect() will collect rdd to master and cerate list of elements and then join it with empty file \n",
    "\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAKE\\tFUELTYPE\\tASPIRE\\tDOORS\\tBODY\\tDRIVE\\tCYLINDERS\\tHP\\tRPM\\tMPG-CITY\\tMPG-HWY\\tPRICE',\n",
       " 'subaru\\tgas\\tstd\\ttwo\\thatchback\\tfwd\\tfour\\t69\\t4900\\t31\\t36\\t5118',\n",
       " 'chevrolet\\tgas\\tstd\\ttwo\\thatchback\\tfwd\\tthree\\t48\\t5100\\t47\\t53\\t5151',\n",
       " 'mazda\\tgas\\tstd\\ttwo\\thatchback\\tfwd\\tfour\\t68\\t5000\\t30\\t31\\t5195',\n",
       " 'toyota\\tgas\\tstd\\ttwo\\thatchback\\tfwd\\tfour\\t62\\t4800\\t35\\t39\\t5348']"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsvData=autoData.map(lambda x : x.replace(\",\",\"\\t\")) #replace() in map()\n",
    "tsvData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyotaData=autoData.filter(lambda x: \"toyota\" in x) # filter()\n",
    "toyotaData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toyota,gas,std,two,hatchback,fwd,four,62,4800,35,39,5348',\n",
       " 'toyota,gas,std,two,hatchback,fwd,four,62,4800,31,38,6338',\n",
       " 'toyota,gas,std,four,hatchback,fwd,four,62,4800,31,38,6488',\n",
       " 'toyota,gas,std,four,wagon,fwd,four,62,4800,31,37,6918',\n",
       " 'toyota,gas,std,four,sedan,fwd,four,70,4800,30,37,6938']"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toyotaData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toyota',\n",
       " 'gas',\n",
       " 'std',\n",
       " 'two',\n",
       " 'hatchback',\n",
       " 'fwd',\n",
       " 'four',\n",
       " '62',\n",
       " '4800',\n",
       " '35',\n",
       " '39',\n",
       " '5348',\n",
       " 'toyota',\n",
       " 'gas',\n",
       " 'std',\n",
       " 'two',\n",
       " 'hatchback',\n",
       " 'fwd',\n",
       " 'four',\n",
       " '62']"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=toyotaData.flatMap(lambda line: line.split(\",\")) #flatMap()\n",
    "#words.count() #only one action at a time\n",
    "words.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "8\n",
      "5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "collData = sc.parallelize([4,3,8,5,8])\n",
    "\n",
    "for numbData in collData.distinct().take(5): # print distinct data ,keep one out of duplicates\n",
    "    print(numbData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collData.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bmw,gas,std,two,sedan,rwd,six,182,5400,16,22,41315'"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoData.reduce(lambda x,y: x if len(x) < len(y) else y) # find shortest line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAKE', 'subaru', 'chevrolet', 'mazda', 'toyota']"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cylData = autoData.map(lambda x: (x.split(\",\")[0], x.split(\",\")[7]))\n",
    "cylData.take(5)\n",
    "cylData.keys().take(5) # action #using take() to view result of action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('subaru', '69')"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Remove header row\n",
    "\n",
    "header = cylData.first()\n",
    "cylHPData= cylData.filter(lambda line: line != header)\n",
    "cylHPData.first() # check if first line is header or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find average by Brand of vehicle##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chevrolet', ('487070', 3)),\n",
       " ('mazda', ('6868686868848484841018410110113512072', 16)),\n",
       " ('mitsubishi', ('686868881028888116116116145145145', 13)),\n",
       " ('nissan', ('696969556969696969699797152152152160160200', 18)),\n",
       " ('dodge', ('686868686810288145', 8))]"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_bykey = cylHPData.combineByKey((lambda x: (x,1)),(lambda x,value:(x[0]+value,x[1]+1)),(lambda x,y:(x[0]+y[0],x[1]+y[1])))\n",
    "comb_bykey.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chevrolet', 162356.66666666666),\n",
       " ('mazda', 4.292929293030303e+35),\n",
       " ('mitsubishi', 5.283606777145293e+31),\n",
       " ('nissan', 3.872053094276094e+40),\n",
       " ('dodge', 8.585858585128602e+16)]"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg=comb_bykey.mapValues(lambda x:float(x[0])/float(x[1]))\n",
    "avg.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get average using reduce function and a user generated function\n",
    "\n",
    "def getMPG(autoStr) :\n",
    "    if isinstance(autoStr, int) : # check if autoStr is integer type ,using isinstance()\n",
    "        return autoStr\n",
    "    attList=autoStr.split(\",\")\n",
    "    if attList[9].isdigit() : # check if attList[9] is [0-9] ,using isdigit()\n",
    "        return int(attList[9])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#find average MPG-City for all cars\n",
    "\n",
    "autoData.reduce(lambda x,y : getMPG(x) + getMPG(y)) / (autoData.count()-1.0)  # suv=btact 1 to not account header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MAKE,FUELTYPE,ASPIRE,4,BODY,DRIVE,CYLINDERS,HP,RPM,MPG-CITY,MPG-HWY,PRICE',\n",
       " 'subaru,gas,std,2,hatchback,FWD,four,69,4900,31,36,5118',\n",
       " 'chevrolet,gas,std,2,hatchback,FWD,three,48,5100,47,53,5151',\n",
       " 'mazda,gas,std,2,hatchback,FWD,four,68,5000,30,31,5195',\n",
       " 'toyota,gas,std,2,hatchback,FWD,four,62,4800,35,39,5348']"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using functions for transformation\n",
    "#cleanse and transform an RDD\n",
    "\n",
    "def cleanseRDD(autoStr) :\n",
    "    if isinstance(autoStr, int) :\n",
    "        return autoStr\n",
    "    attList=autoStr.split(\",\")\n",
    "    \n",
    "    #convert doors to a number str\n",
    "    if attList[3] == \"two\" :\n",
    "         attList[3]=\"2\"\n",
    "    else :\n",
    "         attList[3]=\"4\"\n",
    "    \n",
    "    #Convert Drive to uppercase    \n",
    "    attList[5] = attList[5].upper()\n",
    "    #return attList # when this func will be caleed in map() then every line will be a list with above transformations \n",
    "    \n",
    "    # to display entire rdd as it will be before applying map() i.e when loaded from textfile with above transformations applied,\n",
    "    #then use join()\n",
    "    \n",
    "    return \",\".join(attList)\n",
    "    \n",
    "cleanedData=autoData.map(cleanseRDD)\n",
    "cleanedData.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "universe\n",
      "peace\n",
      "world\n",
      "war\n",
      "\n",
      "\n",
      "peace\n",
      "war\n"
     ]
    }
   ],
   "source": [
    "#Set operations on rdd\n",
    "\n",
    "words1 = sc.parallelize([\"hello\",\"war\",\"peace\",\"world\"])\n",
    "words2 = sc.parallelize([\"war\",\"peace\",\"universe\"])\n",
    "\n",
    "for u in words1.union(words2).distinct().collect(): #remove duplicates after union()\n",
    "    print(u)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for i in words1.intersection(words2).collect(): #intersection() results in common elements\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function that splits the line as well as counts sedans and hatchbacks - using accumulator and broadcast##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Speed optimization##\n",
    "\n",
    "#Initialize accumulator\n",
    "sedanCount = sc.accumulator(0)  #sc.accumulator(<initialvalue>)\n",
    "hatchbackCount =sc.accumulator(0) \n",
    "\n",
    "#Set Broadcast variable\n",
    "\n",
    "sedanText=sc.broadcast(\"sedan\") #sc.broadcast(<broadcastvalue>)\n",
    "hatchbackText=sc.broadcast(\"hatchback\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitLines(line) :\n",
    "\n",
    "    global sedanCount # declare accumulator var as global\n",
    "    global hatchbackCount # declare accumulator var as global\n",
    "\n",
    "    # check if broadcast var.value i.e 'sedan' is in line which is passed as argument ,\n",
    "    #if found then increase accu var.value by 1 else it will remain 0\n",
    "    \n",
    "    if sedanText.value in line: # if 'sedan' in line: (if broadcast and accumulator concept not used)\n",
    "        sedanCount +=1 # to get count of 'sedan' in each line passd as argument\n",
    "        \n",
    "    if hatchbackText.value in line:\n",
    "        hatchbackCount +=1 # to get count of 'hatchback' in each line passd as argument\n",
    "        \n",
    "    return line.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 67\n"
     ]
    }
   ],
   "source": [
    "#do the map\n",
    "splitData=autoData.map(splitLines)\n",
    "\n",
    "splitData.count() # count elements of rdd\n",
    "\n",
    "print(sedanCount, hatchbackCount) # print accu and broad var set using func splitLines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## total amnt per customer id and display smalledt amnt on top##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['44,8602,37.19',\n",
       " '35,5368,65.89',\n",
       " '2,3391,40.64',\n",
       " '47,6694,14.98',\n",
       " '29,680,13.08']"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cust_ord = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\customer-orders.csv\")\n",
    "cust_ord.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLine(line):\n",
    "    fields = line.split(\",\")\n",
    "    return (int(fields[0]), int(float(fields[2]) * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(44, 3719), (35, 6589), (2, 4064), (47, 1498), (29, 1308)]"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = cust_ord.map(parseLine)\n",
    "c.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "customerOrdersSum = c.reduceByKey(lambda x, y: x + y) # get total amnt per cust id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(330937, 45), (379053, 79), (392417, 96), (404260, 23), (417222, 99)]"
      ]
     },
     "execution_count": 481,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customerOrdersSorted = customerOrdersSum.map(lambda x: (x[1], x[0])).sortByKey() #sort amnt\n",
    "customerOrdersSorted.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer id: 45with amnt: 3309.37\n",
      "customer id: 79with amnt: 3790.53\n",
      "customer id: 96with amnt: 3924.17\n",
      "customer id: 23with amnt: 4042.6\n",
      "customer id: 99with amnt: 4172.22\n"
     ]
    }
   ],
   "source": [
    "for i in customerOrdersSorted.take(5):\n",
    "    print('customer id: '+ str(i[1])+'with amnt: '+str(i[0]/100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD can be created from list containing dict,or list or tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Simple DataFrame from a Tuple List##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_list = [('a', 1), ('b', 2), ('c', 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| _1| _2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  c|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(a_list) # schema not given\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|let|num|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  2|\n",
      "|  c|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1=spark.createDataFrame(a_list,['let','num']) # schema given (infer schema)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- let: string (nullable = true)\n",
      " |-- num: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Simple DataFrame from a Dictionary##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dict = [{'letters': 'a', 'numbers': 1},\n",
    "          {'letters': 'b', 'numbers': 2},\n",
    "          {'letters': 'c', 'numbers': 3}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2=spark.createDataFrame(a_dict) # no schema given infer schema) .Warning but still dataframe will be created\n",
    "df2.show() # by default 20 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Simple DataFrame Using a StructType Schema + RDD##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=StructType([StructField('letters', StringType(), True),StructField('numbers', IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd1=sc.parallelize(a_list)\n",
    "df1=spark.createDataFrame(rdd1,schema) # schema programatically gieven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Inspection Functions:##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['letters', 'numbers']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('letters', 'string'), ('numbers', 'int')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(letters,StringType,true),StructField(numbers,IntegerType,true)))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(letters='a', numbers=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(letters='a', numbers=1),\n",
       " Row(letters='b', numbers=2),\n",
       " Row(letters='c', numbers=3)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(5) #default 1 row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(letters='a', numbers=1),\n",
       " Row(letters='b', numbers=2),\n",
       " Row(letters='c', numbers=3)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|summary|letters|numbers|\n",
      "+-------+-------+-------+\n",
      "|  count|      3|      3|\n",
      "|   mean|   null|    2.0|\n",
      "| stddev|   null|    1.0|\n",
      "|    min|      a|      1|\n",
      "|    max|      c|      3|\n",
      "+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Scan ExistingRDD[letters#40,numbers#41]\n"
     ]
    }
   ],
   "source": [
    "df1.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[letters: string, numbers: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df1) # displays type of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      c|      3|\n",
      "|      b|      2|\n",
      "|      a|      1|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1['letters','numbers'].orderBy(col('letters').desc()).show() # A way to sort column in dataframe without using select()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use these functions:##\n",
    "\n",
    "##unionAll()/union(): combine two DataFrames together ##\n",
    "##orderBy()/sort(): perform sorting of DataFrame columns ##\n",
    "##select(): select which DataFrame columns to retain ##\n",
    "##drop(): select a single DataFrame column to remove ##\n",
    "##filter(): retain DataFrame rows that match a condition ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.unionAll(df1).show() # unionAll() produce duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df1.select('letters','numbers').show()\n",
    "#df1.select(col('letters'),col('numbers')).show()\n",
    "#df1.select(df1['letters'],df1['numbers']).show()\n",
    "df1.select(['letters','numbers']).show()  # selecting muliple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df1).show() # union() also produce duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      b|      2|\n",
      "|      a|      1|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df1).distinct().show() # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      b|      2|\n",
      "|      a|      1|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.union(df1).dropDuplicates().show() # remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.orderBy('numbers').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      c|      3|\n",
      "|      b|      2|\n",
      "|      a|      1|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.orderBy(col('numbers').desc()).show() # using col() & desc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      c|      3|\n",
      "|      b|      2|\n",
      "|      a|      1|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.orderBy('numbers',ascending=False).show()# imp to write 'ascending=' instead of just False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.sort(col('numbers')).show() # using sort()\n",
    "\n",
    "df1.sort(col('numbers').desc()).show() # descending using sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.drop('letters').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      b|      2|\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here is some numeric filtering with comparison operators\n",
    "# (>, <, >=, <=, ==, != all work)\n",
    "\n",
    "df1.filter(df1.numbers>1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      b|      2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter((df1.numbers>1) & (df1.numbers<3)).show() # multiple conditions in individual brackets\n",
    "\n",
    "# use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      a|      1|\n",
      "|      b|      2|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(col('letters').isin(['a', 'b'])).show() # alternate way of filtering data using isin() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|letters|numbers|\n",
      "+-------+-------+\n",
      "|      c|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.filter(col('letters').isin(['a', 'b'])==False).show() # to implement is not in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using groupBy(): ##\n",
    "    \n",
    "#count(): counts the number of records for each group#\n",
    "#sum(): compute the sum for each numeric column for each group#\n",
    "#min(): computes the minimum value for each numeric column for each group#\n",
    "#max(): computes the maximum value for each numeric column for each group#\n",
    "#avg() or mean(): computes average values for each numeric columns for each group#\n",
    "#pivot(): pivots a column of the current DataFrame and perform the specified aggregation#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "nycflights_schema = StructType([\n",
    "  StructField('year', IntegerType(), True),\n",
    "  StructField('month', IntegerType(), True),\n",
    "  StructField('day', IntegerType(), True),\n",
    "  StructField('dep_time', StringType(), True),\n",
    "  StructField('dep_delay', IntegerType(), True),\n",
    "  StructField('arr_time', StringType(), True),\n",
    "  StructField('arr_delay', IntegerType(), True),\n",
    "  StructField('carrier', StringType(), True),\n",
    "  StructField('tailnum', StringType(), True),\n",
    "  StructField('flight', StringType(), True),  \n",
    "  StructField('origin', StringType(), True),\n",
    "  StructField('dest', StringType(), True),\n",
    "  StructField('air_time', IntegerType(), True),\n",
    "  StructField('distance', IntegerType(), True),\n",
    "  StructField('hour', IntegerType(), True),\n",
    "  StructField('minute', IntegerType(), True)\n",
    "  ])\n",
    "\n",
    "nycflights = \\\n",
    "(spark\n",
    " .read\n",
    " .format('csv')\n",
    " .options(header = True,inferSchema=True) # if inferSchema-=True is not specified then spark will read all column as string type column\n",
    " .load('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\nycflights13.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|year|month|day|dep_time|dep_delay|arr_time|arr_delay|carrier|tailnum|flight|origin|dest|air_time|distance|hour|minute|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "|2013|    1|  1|     517|        2|     830|       11|     UA| N14228|  1545|   EWR| IAH|     227|    1400|   5|    17|\n",
      "|2013|    1|  1|     533|        4|     850|       20|     UA| N24211|  1714|   LGA| IAH|     227|    1416|   5|    33|\n",
      "|2013|    1|  1|     542|        2|     923|       33|     AA| N619AA|  1141|   JFK| MIA|     160|    1089|   5|    42|\n",
      "|2013|    1|  1|     544|       -1|    1004|      -18|     B6| N804JB|   725|   JFK| BQN|     183|    1576|   5|    44|\n",
      "|2013|    1|  1|     554|       -6|     812|      -25|     DL| N668DN|   461|   LGA| ATL|     116|     762|   5|    54|\n",
      "|2013|    1|  1|     554|       -4|     740|       12|     UA| N39463|  1696|   EWR| ORD|     150|     719|   5|    54|\n",
      "|2013|    1|  1|     555|       -5|     913|       19|     B6| N516JB|   507|   EWR| FLL|     158|    1065|   5|    55|\n",
      "|2013|    1|  1|     557|       -3|     709|      -14|     EV| N829AS|  5708|   LGA| IAD|      53|     229|   5|    57|\n",
      "|2013|    1|  1|     557|       -3|     838|       -8|     B6| N593JB|    79|   JFK| MCO|     140|     944|   5|    57|\n",
      "|2013|    1|  1|     558|       -2|     753|        8|     AA| N3ALAA|   301|   LGA| ORD|     138|     733|   5|    58|\n",
      "|2013|    1|  1|     558|       -2|     849|       -2|     B6| N793JB|    49|   JFK| PBI|     149|    1028|   5|    58|\n",
      "|2013|    1|  1|     558|       -2|     853|       -3|     B6| N657JB|    71|   JFK| TPA|     158|    1005|   5|    58|\n",
      "|2013|    1|  1|     558|       -2|     924|        7|     UA| N29129|   194|   JFK| LAX|     345|    2475|   5|    58|\n",
      "|2013|    1|  1|     558|       -2|     923|      -14|     UA| N53441|  1124|   EWR| SFO|     361|    2565|   5|    58|\n",
      "|2013|    1|  1|     559|       -1|     941|       31|     AA| N3DUAA|   707|   LGA| DFW|     257|    1389|   5|    59|\n",
      "|2013|    1|  1|     559|        0|     702|       -4|     B6| N708JB|  1806|   JFK| BOS|      44|     187|   5|    59|\n",
      "|2013|    1|  1|     559|       -1|     854|       -8|     UA| N76515|  1187|   EWR| LAS|     337|    2227|   5|    59|\n",
      "|2013|    1|  1|     600|        0|     851|       -7|     B6| N595JB|   371|   LGA| FLL|     152|    1076|   6|     0|\n",
      "|2013|    1|  1|     600|        0|     837|       12|     MQ| N542MQ|  4650|   LGA| ATL|     134|     762|   6|     0|\n",
      "|2013|    1|  1|     601|        1|     844|       -6|     B6| N644JB|   343|   EWR| PBI|     147|    1023|   6|     1|\n",
      "+----+-----+---+--------+---------+--------+---------+-------+-------+------+------+----+--------+--------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nycflights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dep_time: integer (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- arr_time: integer (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      " |-- carrier: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- flight: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- air_time: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- minute: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nycflights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: int, month: int, day: int, dep_time: int, dep_delay: int, arr_time: int, arr_delay: int, carrier: string, tailnum: string, flight: int, origin: string, dest: string, air_time: int, distance: int, hour: int, minute: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(nycflights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|month|count|\n",
      "+-----+-----+\n",
      "|   12|28135|\n",
      "|    1|27004|\n",
      "|    6|28243|\n",
      "|    3|28834|\n",
      "|    5|28796|\n",
      "|    9|27574|\n",
      "|    4|28330|\n",
      "|    8|29327|\n",
      "|    7|29425|\n",
      "|   10|28889|\n",
      "|   11|27268|\n",
      "|    2|24951|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nycflights.groupby('month').count().show() # creates a new column with aggregate `count` values and groupBy Column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+------------------+\n",
      "|month|max(arr_delay)|    avg(dep_delay)|\n",
      "+-----+--------------+------------------+\n",
      "|   12|           878|16.576687569162672|\n",
      "|    1|          1272|10.036665030396858|\n",
      "|    6|          1127|20.846331791143424|\n",
      "|    3|           915|13.227076109105209|\n",
      "|    5|           875|12.986859348988771|\n",
      "|    9|          1007|6.7224762185679525|\n",
      "|    4|           931|13.938037741305763|\n",
      "|    8|           490|12.611039839117922|\n",
      "|    7|           989|21.727786554326837|\n",
      "|   10|           688| 6.243988413080655|\n",
      "|   11|           796|  5.43536156833734|\n",
      "|    2|           834|10.816842549598986|\n",
      "+-----+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nycflights.groupby('month').agg({'dep_delay': 'avg', 'arr_delay': 'max'}).show() # multiple aggreagtion func on diff columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+-----+\n",
      "|month|origin|dest|count|\n",
      "+-----+------+----+-----+\n",
      "|    1|   JFK| LAX|  937|\n",
      "|    1|   LGA| ATL|  878|\n",
      "|    1|   JFK| SFO|  671|\n",
      "|    1|   LGA| ORD|  583|\n",
      "|    1|   EWR| ORD|  502|\n",
      "|    1|   JFK| BOS|  486|\n",
      "|    1|   JFK| MCO|  456|\n",
      "|    1|   LGA| MIA|  451|\n",
      "|    1|   JFK| FLL|  439|\n",
      "|    1|   LGA| DFW|  437|\n",
      "+-----+------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nycflights.groupby('month', 'origin', 'dest').count().orderBy('month', 'count',ascending = [1, 0]).show(10)\n",
    "\n",
    "#nycflights.groupby(['month', 'origin', 'dest']).count().orderBy(['month', 'count'],ascending = [1, 0]).show(10)\n",
    "\n",
    "# group by on multiple columns                          \n",
    "# perform a 'count' aggregation on the groups\n",
    "#orderBY on multiple col with diff sorting order for each col\n",
    "#ascending=[1,0] means ascending is true for 'month' col and false(i.e descending) for 'count' col.\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: int, count: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "  nycflights\n",
    "  .groupBy('month')\n",
    "  .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|carrier|               EWR|               JFK|               LGA|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|     UA| 12.52286865854727|               7.9|12.087916294500447|\n",
      "|     AA|10.035419126328216|10.302155109221522| 6.705769103100312|\n",
      "|     EV| 20.16493117893477|18.520361990950228| 19.12549969715324|\n",
      "|     B6|13.100262224278882|12.757453126122458|14.805738396624472|\n",
      "|     DL|12.084592145015106| 8.333187709334497|  9.57299733123332|\n",
      "|     OO|20.833333333333332|              null|10.434782608695652|\n",
      "|     F9|              null|              null|20.215542521994134|\n",
      "|     YV|              null|              null|18.996330275229358|\n",
      "|     US| 3.735103926096998| 5.866958571909734|3.3065054875139177|\n",
      "|     MQ|17.467267552182165|13.199970870958346| 8.528568781271234|\n",
      "|     HA|              null| 4.900584795321637|              null|\n",
      "|     AS| 5.804775280898877|              null|              null|\n",
      "|     FL|              null|              null| 18.72607467838092|\n",
      "|     VX|11.927377892030849|13.279440559440559|              null|\n",
      "|     WN|17.864376130198917|              null|            17.557|\n",
      "|     9E| 5.951666666666667|19.001516902629298| 8.894182124789207|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x=nycflights.groupBy('carrier').pivot('origin').avg('dep_delay') # groupBy() with pivot() and aggregation.\n",
    "\n",
    "x.show()\n",
    "\n",
    "# pivot(col name)- will produce pivot col as oe col grouped values as one column and pivot unique column values \n",
    "# null will be the value fof pivot col if aggregation can't be done\n",
    "#as different columns having calculate aggregation accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Operations ##\n",
    "\n",
    "format_number(): apply formatting to a number, rounded to d decimal places, and return the result as a string\n",
    "when() & otherwise(): when() evaluates a list of conditions and returns one of multiple possible result expressions; if otherwise() is not invoked, None is returned for unmatched conditions\n",
    "concat_ws(): concatenates multiple input string columns together into a single string column, using the given separator\n",
    "to_utc_timestamp(): assumes the given timestamp is in given timezone and converts to UTC\n",
    "year(): extracts the year of a given date as integer\n",
    "month(): extracts the month of a given date as integer\n",
    "dayofmonth(): extracts the day of the month of a given date as integer\n",
    "hour(): extract the hour of a given date as integer\n",
    "minute(): extract the minute of a given date as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
