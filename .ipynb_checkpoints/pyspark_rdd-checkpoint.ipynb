{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "       .appName(\"Pyspark\")\\\n",
    "       .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data) # to create RDD on an iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.reduce(lambda a, b: a + b) # reduce function will aggregate elements of rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "distFile = sc.textFile(\"data.txt\") # load rdd from textfile\n",
    "#he textFile method also takes an optional second argument for controlling the number of partitions of the file. \n",
    "#By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS),\n",
    "#but you can also ask for a higher number of partitions by passing a larger value.\n",
    "#Note that you cannot have fewer partitions than blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (2, 'aa'), (3, 'aaa')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x)).collect() # map function operates on each element of rdd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default, each transformed RDD may be recomputed each time you run an action on it. \n",
    "#However, you may also persist an RDD in memory using the persist (or cache) method, \n",
    "#in which case Spark will keep the elements around on the cluster for much faster access the next time you query it. \n",
    "#There is also support for persisting RDDs on disk, or replicated across multiple nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5, 6, 7])\n",
    "nums.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered1 = nums.filter(lambda x : x % 2 == 1)\n",
    "filtered1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered2 = nums.filter(lambda x : x % 2 == 0)\n",
    "filtered2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is my test file']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\mytest.txt\").take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k1,v1', 'k1,v2', 'k2,v3', 'k2,v4', 'k3,v7']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\mytest.txt\")\n",
    "R.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['k1,v11', 'k1,v22', 'k1,v33', 'k2,v55', 'k4,v77']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S= sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\mytest1.txt\")\n",
    "S.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['k1', 'v1'], ['k1', 'v2'], ['k2', 'v3'], ['k2', 'v4'], ['k3', 'v7']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1=R.map(lambda x:x.split(','))\n",
    "r1.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 'v1'),\n",
       " ('k1', 'v2'),\n",
       " ('k2', 'v3'),\n",
       " ('k2', 'v4'),\n",
       " ('k3', 'v7'),\n",
       " ('k3', 'v8'),\n",
       " ('k3', 'v9')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#r2 = r1.map(lambda s: (s[0],s[1]))\n",
    "#r2.take(10)\n",
    "\n",
    "r2 = r1.flatMap(lambda s: [(s[0],s[1])])\n",
    "r2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['k1', 'v11'], ['k1', 'v22'], ['k1', 'v33'], ['k2', 'v55'], ['k4', 'v77']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = S.map(lambda s: s.split(\",\"))\n",
    "s1.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 'v11'),\n",
       " ('k1', 'v22'),\n",
       " ('k1', 'v33'),\n",
       " ('k2', 'v55'),\n",
       " ('k4', 'v77'),\n",
       " ('k5', 'v88')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2 = s1.flatMap(lambda s: [(s[0],s[1])])\n",
    "s2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', ('v1', 'v11')),\n",
       " ('k1', ('v1', 'v22')),\n",
       " ('k1', ('v1', 'v33')),\n",
       " ('k1', ('v2', 'v11')),\n",
       " ('k1', ('v2', 'v22')),\n",
       " ('k1', ('v2', 'v33')),\n",
       " ('k2', ('v3', 'v55')),\n",
       " ('k2', ('v4', 'v55'))]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RjoinedS = r2.join(s2)\n",
    "RjoinedS.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum2 = nums.map(lambda x: x + 2)\n",
    "sum2.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq = nums.map(lambda x: x * x)\n",
    "sq.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = sc.parallelize([1, 2, 3, 4])\n",
    "n.reduce(lambda x, y: x * y)\n",
    "#n.fold(1,(lambda x, y: x * y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 1), ('c', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "sorted(rdd1.map(lambda x: (x, 1)).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 1, 'a', 1, 'c', 1]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "rdd1.flatMap(lambda x: (x, 1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crazy crazy fox jumped',\n",
       " 'crazy fox jumped',\n",
       " 'fox is fast',\n",
       " 'fox is smart',\n",
       " 'dog is smart']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile('file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\mytest3.txt')\n",
    "lines.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crazy',\n",
       " 'crazy',\n",
       " 'fox',\n",
       " 'jumped',\n",
       " 'crazy',\n",
       " 'fox',\n",
       " 'jumped',\n",
       " 'fox',\n",
       " 'is',\n",
       " 'fast']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.flatMap(lambda x: x.split(' ')).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crazy', 1),\n",
       " ('crazy', 1),\n",
       " ('fox', 1),\n",
       " ('jumped', 1),\n",
       " ('crazy', 1),\n",
       " ('fox', 1),\n",
       " ('jumped', 1),\n",
       " ('fox', 1),\n",
       " ('is', 1),\n",
       " ('fast', 1)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assigninitial = lines.flatMap(lambda x: x.split(' ')).map(lambda w:(w,1))\n",
    "assigninitial.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crazy', 3),\n",
       " ('is', 3),\n",
       " ('smart', 2),\n",
       " ('fox', 4),\n",
       " ('jumped', 2),\n",
       " ('fast', 1),\n",
       " ('dog', 1)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies=assigninitial.reduceByKey(lambda x,y:x+y)\n",
    "x=frequencies.take(10)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crazy', 'is', 'smart', 'fox', 'jumped', 'fast', 'dog']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies.keys().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('crazy', 3), 0),\n",
       " (('is', 3), 1),\n",
       " (('smart', 2), 2),\n",
       " (('fox', 4), 3),\n",
       " (('jumped', 2), 4),\n",
       " (('fast', 1), 5),\n",
       " (('dog', 1), 6)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies.zipWithIndex().take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequencies.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('crazy', 3),\n",
       " ('dog', 1),\n",
       " ('fast', 1),\n",
       " ('fox', 4),\n",
       " ('is', 3),\n",
       " ('jumped', 2),\n",
       " ('smart', 2)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('smart', 2),\n",
       " ('jumped', 2),\n",
       " ('is', 3),\n",
       " ('fox', 4),\n",
       " ('fast', 1),\n",
       " ('dog', 1),\n",
       " ('crazy', 3)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies.sortByKey(ascending=False).collect() #frequencies.sortByKey(False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortBy(lambda x: x[0]).collect() #sortBy() needs key func as argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3),\n",
       " ('fleece', 7),\n",
       " ('had', 2),\n",
       " ('little', 4),\n",
       " ('lamb', 5),\n",
       " ('Mary', 1),\n",
       " ('whose', 6),\n",
       " ('was', 8),\n",
       " ('white', 9)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "\n",
    "sc.parallelize(tmp2).sortByKey(True,1,keyfunc=lambda k: k[0].lower()).collect() # sc.parallelize(tmp2).sortByKey(True,1,keyfunc=lambda k: k.lower()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = sc.parallelize([1, 2, 3, 4])\n",
    "sum1 = numbers.fold(0, (lambda x, y: x + y))#alt way fr addition of all elements\n",
    "print(sum1)\n",
    "mult1=numbers.fold(1, (lambda x, y: x * y))#way fr multiplication of all elements\n",
    "mult1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers = sc.parallelize([1, 2, 3, 4])\n",
    "sum1 = numbers.sum() #numbers.subtract()\n",
    "sum1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1= [('k1', 1), ('k2', 2), ('k3', 5)]\n",
    "d2= [('k1', 3), ('k2',4), ('k4', 8)]\n",
    "rdd1 = sc.parallelize(d1)\n",
    "rdd2 = sc.parallelize(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 1), ('k2', 2), ('k3', 5), ('k1', 3), ('k2', 4), ('k4', 8)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3=rdd1.union(rdd2)\n",
    "rdd3.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 4), ('k3', 5), ('k2', 6), ('k4', 8)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3.reduceByKey(lambda x,y: x+y).take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['crazy crazy fox jumped',\n",
       " 'crazy fox jumped',\n",
       " 'fox is fast',\n",
       " 'fox is smart',\n",
       " 'dog is smart']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\mytest3.txt\")\n",
    "lines.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['crazy', 'crazy', 'fox', 'jumped'],\n",
       " ['crazy', 'fox', 'jumped'],\n",
       " ['fox', 'is', 'fast'],\n",
       " ['fox', 'is', 'smart'],\n",
       " ['dog', 'is', 'smart']]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(\"file:///D:\\\\spark-2.3.2-bin-hadoop2.7\\\\mytest3.txt\")\n",
    "\n",
    "r1 = lines.map(lambda s : s.split(\" \")).collect()\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('crazy', 'crazy'), 1),\n",
       " (('crazy', 'fox'), 1),\n",
       " (('fox', 'jumped'), 1),\n",
       " (('crazy', 'fox'), 1),\n",
       " (('fox', 'jumped'), 1),\n",
       " (('fox', 'is'), 1),\n",
       " (('is', 'fast'), 1),\n",
       " (('fox', 'is'), 1),\n",
       " (('is', 'smart'), 1),\n",
       " (('dog', 'is'), 1)]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = lines.map(lambda s : s.split(\" \")).flatMap(lambda s: [((s[i],s[i+1]),1) for i in range(len(s)-1)]).take(10)                                                                                                        \n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('crazy', 'crazy'), 1),\n",
       " (('fox', 'jumped'), 2),\n",
       " (('is', 'smart'), 2),\n",
       " (('crazy', 'fox'), 2),\n",
       " (('fox', 'is'), 2),\n",
       " (('is', 'fast'), 1),\n",
       " (('dog', 'is'), 1)]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f=lines.map(lambda s : s.split(\" \")).flatMap(lambda s: [((s[i],s[i+1]),1) for i in range(len(s)-1)]).reduceByKey(lambda x,y:x+y)\n",
    "f.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abc', 'de'), ('xyz', 'deeee'), ('abc', 'de'), ('xyz', 'bababa')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = [\"abc,de\", \"xyz,deeee,ze\", \"abc,de,ze,pe\", \"xyz,bababa\"]\n",
    "rdd4 = sc.parallelize(data2)\n",
    "rdd5 = rdd4.map(lambda x : (x.split(\",\")[0],x.split(\",\")[1])) # rdd5 = rdd4.map(lambda x : tuple(x.split(\",\")))\n",
    "rdd5.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 7]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = [10, 1, 2, 9, 3, 4, 5, 6, 7]\n",
    "sc.parallelize(nums).top(3) #top (in desc order) n elements\n",
    "#sc.parallelize(nums).sortBy(lambda x:-x).take(3)# sc.parallelize(nums).sortBy(lambda x:x,False).take(3)\n",
    "#sc.parallelize(nums).takeOrdered(3, lambda x: -x) #sc.parallelize(nums).takeOrdered(3, key=lambda x: -x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = [10, 1, 2, 9, 3, 4, 5, 6, 7]\n",
    "sc.parallelize(nums).takeOrdered(3) #ascending by default\n",
    "#sc.parallelize(nums).sortBy(lambda x:x).take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'z2'), (2, 'z3'), (3, 'z5')]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv = [(10,\"z1\"), (1,\"z2\"), (2,\"z3\"), (9,\"z4\"), (3,\"z5\"), (4,\"z6\"), (5,\"z7\"), (6,\"z8\"), (7,\"z9\")]\n",
    "sc.parallelize(kv).takeOrdered(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 'z1'), (9, 'z4'), (7, 'z9')]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(kv).takeOrdered(3, lambda x:-x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10, 'z1'), (9, 'z4'), (7, 'z9')]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize(kv).top(3) #top (in desc order by key) n elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = [(\"k1\", 1), (\"k1\", 2), (\"k1\", 3), (\"k1\", 4), (\"k1\", 5), \n",
    "             (\"k2\", 6), (\"k2\", 7), (\"k2\", 8), \n",
    "             (\"k3\", 10), (\"k3\", 12)]\n",
    "rdd_c=sc.parallelize(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', (15, 5)), ('k2', (21, 3)), ('k3', (22, 2))]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddc=rdd_c.combineByKey((lambda v:(v,1)),(lambda x,v1:(x[0]+v1,x[1]+1)),(lambda x,y:(x[0]+y[0],x[1]+y[1])))\n",
    "rddc.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('k1', 3.0), ('k2', 7.0), ('k3', 11.0)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddc.mapValues(lambda v : v[0]/float(v[1])).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard deviation using combinebykey():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (15.0, 101.0, 3)), ('B', (30.0, 500.0, 2)), ('Z', (28.0, 242.0, 4))]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=[ ('A', 2.0),  ('A', 4.0),  ('A', 9.0),  ('B', 10.0),  ('B', 20.0),  ('Z', 3.0), ('Z', 5.0),  ('Z', 8.0),  ('Z', 12.0)]\n",
    "rdd = sc.parallelize( data )\n",
    "\n",
    "sumCount = rdd.combineByKey(lambda value: (value, value*value, 1),lambda x, value: (x[0] + value, x[1] + value*value, x[2] + 1),lambda x, y: (x[0] + y[0], x[1] + y[1], x[2] + y[2]))\n",
    "\n",
    "sumCount.take(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', (5.0, 2.943920288775949)),\n",
       " ('B', (15.0, 5.0)),\n",
       " ('Z', (7.0, 3.391164991562634))]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def  stdDev( sumX, sumSquared, n ):\n",
    "                mean = sumX / n\n",
    "                stdDeviation = math.sqrt ((sumSquared - n*mean*mean) /n)\n",
    "                return (mean, stdDeviation)\n",
    "\n",
    "\n",
    "meanAndStdDev = sumCount.mapValues(lambda x : stdDev(x[0], x[1], x[2]))\n",
    "meanAndStdDev.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(iterator):\n",
    "    for x in iterator:\n",
    "        print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    print(x)\n",
    "sc.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [10, 20, 3, 4, 5, 2, 2, 20, 20, 10]\n",
    "rdd = sc.parallelize(data, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 3, 4, 5, 2, 2, 20, 20, 10]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serialized 1x Replicated\n"
     ]
    }
   ],
   "source": [
    "print(rdd.getStorageLevel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 20, 3], [4, 5, 2], [2, 20, 20, 10]]"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect() # to combine elements in different partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min and max of elements using mapPartitions():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(iterator):\n",
    "    i = 0\n",
    "    for x in iterator:\n",
    "        if i == 0:\n",
    "            min1 = x\n",
    "            max1 = x\n",
    "            i = 1\n",
    "        else:\n",
    "            if x > max1:\n",
    "                max1 = x\n",
    "            if x < min1:\n",
    "                min1 = x\n",
    "    return (min1, max1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 20, 2, 5, 2, 20]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmaxlist = rdd.mapPartitions(minmax).collect() #get min and max of each partition\n",
    "minmaxlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(minmaxlist)  # Min of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(minmaxlist)   # Max of elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
